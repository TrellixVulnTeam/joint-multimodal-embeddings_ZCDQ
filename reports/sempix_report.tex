\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

\usepackage{natbib}
\bibliographystyle{abbrvnat}
\setcitestyle{authoryear,open={(},close={)}}

% commands for comments
\usepackage[dvipsnames]{xcolor}
\newcommand{\todo}[1]{\textbf{\textcolor{Red}{(TODO: #1)}}}

% ready for submission
%\usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%  \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
%     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[colorlinks,citecolor=blue]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{amsmath}
\usepackage{dcolumn}
\usepackage{makecell}
\usepackage{amssymb}
\usepackage{graphicx}		% include figures
\usepackage{tabularx}
\usepackage{float}			% keeps figures in place
\usepackage{xurl}           % keeps URL from overflowing
\usepackage{caption}
\captionsetup[table]{skip=10pt} % includes space between table and caption
\usepackage{subcaption}		% create subfigures

\newcolumntype{d}[1]{D{.}{.}{#1}} % "decimal" column type
\renewcommand{\ast}{{}^{\textstyle *}} % for raised "asterisks"


\title{Comparing Multimodal Representations in Co-Attention-Based Models for Visual Question Answering}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
   Laura Kopf%\thanks{Use footnote for providing further information
    %about author (webpage, alternative address)---\emph{not} for acknowledging
    %funding agencies.} \\
  %Department of Computer Science\\
  %Cranberry-Lemon University\\
  %Pittsburgh, PA 15213 \\
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  \And
   Patrick Kahardipraja \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

%general question: should we write integers in text as "1" or "one"? I think we have a mixture at this moment.

\begin{abstract}
Multimodal vision and language tasks such as visual question answering (VQA) and visual referring expression are challenging, because they require both the semantic understanding of image content and natural language. To solve multimodal problems, it is crucial to represent information from multiple modalities in a meaningful fashion. %One way to do this is to project joint representations to the same space using all of the modalities as input to establish inter-modal relationships. 
In this work, we investigate the properties of joint multimodal representations derived from a closely similar architectures -- %a closely similar
a task-specific model (MCAN \citep{yu2019mcan}) and a multi-task model (multi-task ViLBERT \citep{lu2020multitask}) -- with different training objective and information streams. %In an effort to gain more insight into the two different architectures, we will compare the multi-task Vision-and-Language BERT (ViLBERT) \citep{lu2020multitask} models to the task-specific deep Modular Co-Attention Network (MCAN) \citep{yu2019mcan} on a VQA task and evaluate grounding. 
Our experimental results suggest that both architectures improve the joint multimodal representations in an orthogonal direction. On the one hand, we show that bidirectional information streams and multi-task training in multi-task ViLBERT assists with learning underlying associations between language and visual concepts. On the other hand, we demonstrate that the attention reduction module in MCAN helps in summarizing information from multi-headed attention heads and improves conceptual groundings.
Our code is publicly available at GitHub.\footnote{\url{https://github.com/lkopf/joint-multimodal-embeddings}}. %something add to more on result? maybe what mcan and vilbert improve?
\end{abstract}

\section{Introduction}
% in recent years models have been developed -> BERT -> rise of general architectures in V&L -> MCAN & ViLBERT (most similar)
% approach, bring to similar level -> evaluate, grounding
% summary of findings
Multimodal representation of language and vision plays a key role in many language and vision tasks such as image-text retrieval \citep{wang2016retrieval}, visual commonsense reasoning \citep{zellers2019vcr}, visual entailment \citep{xie2019entailment} and visual question answering \citep{antol2015vqa}. It depicts a concept from different perspectives, which is usually complementary or supplementary in contents and therefore more informative than unimodal data. In our work, we focus on joint multimodal representations learned by visual question answering models. Visual question answering (VQA) stands out as particularly challenging compared to other language and vision tasks, as it also involves solving many subtasks like object detection, activity recognition, knowledge base reasoning, and commonsense reasoning. As such, the learned multimodal representation is rich in visual and linguistic information. 


%In recent years there have been significant advancements in several language and vision tasks such as image-text retrieval \citep{wang2016retrieval}, visual commonsense reasoning \citep{zellers2019vcr}, visual entailment \citep{xie2019entailment} and visual question answering \citep{antol2015vqa, malinowski2014vqa, ban, zhao2018vqa}. In its most common form, the VQA task requires an algorithm to provide the correct answer for a natural language question asked about an input image. Solving the VQA task stands out as particularly challenging, because it also involves solving many subtasks like object detection, activity recognition, knowledge base reasoning, and commonsense reasoning. 

A variety of models have been developed to solve VQA, mainly utilizing various attention methods such as stacked attention networks \citep{yang2016vqa}, bottom-up and top-down attention mechanism \citep{Anderson_2018_CVPR}, and compositional attention networks \citep{hudson2018mac}. A wave of recent work has also tried to learn textual and visual attention simultaneously through a co-attention mechanism. Such line of work \citep{yu2019mcan, Nguyen_2018_CVPR} demonstrated significantly better performance in VQA. 

Recently, BERT \citep{devlin-etal-2019-bert} has achieved state-of-the-art results on a multitude of NLP tasks. This leads to substantial interest in general architectures to learn joint representations of language and visual content, which can be fine-tuned on downstream tasks (ViLBERT \citep{lu2019vilbert}, VL-BERT \citep{Su2020VL-BERT}, LXMERT \citep{tan2019lxmert}). However, the need to fine-tune on downstream tasks still results in a collection of independent task-specific models instead of a single universal model. \citet{lu2020multitask} extended ViLBERT further with multi-task training on 12 different datasets, which results in a strong performance across a diverse set of language and vision tasks, even without further fine-tuning.

In our work, we aim to investigate the joint multimodal representation by comparing MCAN, which is a task-specific model, against multi-task ViLBERT to examine the characteristics of the learned representation. The architectures of MCAN and multi-task ViLBERT are closely similar, with the main difference being that MCAN only allows a single modality to guide another modality, while multi-task ViLBERT allows both modalities to exchange information simultaneously. Another difference lies in the multimodal fusion method, as MCAN learns to project textual and image representations to a shared space and sum them. Multi-task ViLBERT on the other hand applies an element-wise product between visual and linguistic representations. We intend to find out how these differences affect the properties of the learned multimodal representations. Additionally, we compare the grounding capabilities of both models to investigate to what extend they ground their reasoning in the image, as opposed to learning superficial correlations in the training data. In order to answer these questions, we apply BERT as embeddings and as encoder to MCAN, replacing GloVe \citep{pennington2014glove} (with LSTM encoder) and bring it on a similar footing to multi-task ViLBERT with respect to language representation and train them on VQA 2.0 \citep{goyal2017vqa2} and GQA \citep{hudson2019gqa}. For the grounding comparison, we use the grounding metric available on GQA.

Our experimental results show that the representation learned by multi-task ViLBERT is better at associating the semantic relationship between language and visual concepts. This is demonstrated by its stellar performance on VQA 2.0 and GQA.  However, MCAN outperforms multi-task ViLBERT on the GQA grounding metric. This finding suggests that the attention reduction module in MCAN, which does not exist in multi-task ViLBERT, assists the model to learn better grounding capabilities by summarizing the information in multi-headed attention, and refocuses the model to the most salient information for visual question answering. Furthermore, it suggests that the learned representation by ViLBERT is weakly grounded on perception. We also show that using BERT only offers a marginal performance increase over GloVe in general.
% vilbert perform the best for metric
% mcan performs the best for grounding
% attention reduction module
% bert only increase performance by a bit

% \todo{find better transition}
% A variety of models have been developed to solve this task, using different attention methods such as stacked attention networks \citep{yang2016vqa}, bottom-up and top-down attention mechanism \citep{Anderson_2018_CVPR}, and compositional attention networks \citep{hudson2018mac} to name a few. In our project we will compare two attention-based models that have different approaches to solving the VQA problem: MCAN and ViLBERT.




%MCAN consists of Modular Co-Attention (MCA) layers cascaded in depth. Each layer is composed of two basic attention units, self-attention of questions and images, as well as the guided-attention of images. The input question is transformed into GloVe word embeddings and subsequently passed through a one layer LSTM network. In the multimodal fusion textual and image representations are jointly embedded into the same space and fed into a classifier which predicts the final answer.

%ViLBERT is fairly similar to the MCAN architecture, but in contrast is not task specific. Multi-task ViLBERT is a pre-trained model that is extending the BERT language model \citep{devlin-etal-2019-bert} to jointly represent images and text. It processes visual and textual inputs in separate streams that interact through co-attentional transformer layers. Multi-task ViLBERT is trained jointly on 12 datasets and then put to four vision-and-language tasks: visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval. \citet{lu2020multitask} argue that joint training can improve the performance compared to single-task training with the same architecture. 

%In comparison to MCAN, only one modality can be used to guide another modality while ViLBERT allows both modalities to exchange information simultaneously. Another difference lies in the fusing method the architectures use. MCAN learns to project textual and image representations to a shared space, while ViLBERT applies an element-wise product between visual and linguistic representations.

%It still remains unclear to which extent the models that solve the VQA task understand visual-language concepts. \citet{agrawal12018gvqa} argue that VQA models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To gain an insight into what the models are learning, we focus on whether the models are able to ground their answers to the questions accurately. The GQA dataset \citep{hudson2019gqa} seeks to address the shortcomings of previous VQA datasets and also includes a metric to evaluate grounding.

%In our work, we will investigate the properties of joint multimodal representations derived from both a task-specific model and a multi-task model with respect to different training objective and information streams.
%In order to better understand the influence that both models have on the properties of the representations, we two different models on a VQA task. We build our experimental setup on two pre-existing models and will conduct two modifications to the MCAN. One method will be to replace GloVe as embedding with BERT, and the second method will be to replace the LSTM question encoder with a BERT encoder. We finetune both methods and analyze how this affects the model’s performance. We train and evaluate the modified MCAN architectures on the benchmark VQA 2.0 dataset \citep{goyal2017vqa2} and and compare it to the results of both to the results of the original MCAN architecture and multi-task ViLBERT. In order to evaluate grounding we will evaluate the MCAN architectures on GQA and compare these results to ViLBERT based on multi-task learning on GQA \citep{lu2020multitask}. \todo{Give short preview of our results. Add missing steps in our experimantal setup/evaluation method}
%\todo{General note: revise Introduction according to our report: add/shorten paragraphs where needed, put more focus on grounding and its importance for our results}

\section{Related Work}
\subsection{Advances in Multimodal Embeddings} \label{subsection:embeds}
%bilinear?
Modality refers to to the way in which something happens or is experienced. %Something is multimodal, when it includes multiple such modalities. Taking a walk in the forest can be a multimodal experience: we see trees, hear the wind, smell wood and feel the earth underneath us. 
A research problem or dataset is characterized as multimodal when it includes multiple modalities such as language (written or spoken), vision (images, videos) or vocal (sounds and para-verbal expressions). The aim of multimodal machine learning is to build models that can process and relate information from multiple modalities. The flow of multimodal information is different depending on the multimodal tasks and model architecture. Multimodal machine learning is a multidisciplinary field with a wide range of application areas such as speech recognition, event detection, emotion recognition, media description, multimedia retrieval and multimedia generation. These applications are faced with challenges such as varying levels of noise and conflicts between modalities. In this section we will mainly focus on the challenges of multimodal representation and multimodal fusion and discuss their advancements.

\subsubsection{Multimodal Representations}

The challenge of multimodal representation is to learn how to represent and summarize multimodal data in a meaningful way. In order for a computational model to process data, the data first has to be transformed into a format that can be easily processed. The most commonly used format is a vector or tensor representation of an entity referring to a representation or feature. This entity can be an image, audio sample, individual word, or a sentence. There are many challenges that come with representing multiple modalities: combining data from heterogeneous sources, handling missing data and dealing with different levels of noise. Having good representations is crucial for the performance of machine learning problems. Some properties for good performance are smoothness, sparsity, temporal and spatial coherence, and natural clustering among others \citep{bengio2013represent}. Most unimodal representations nowadays are data-driven, meaning that they are learned from data using neural architectures and not hand-designed for specific applications (e.g.,\ image features from convolutional neural networks (CNN) \citep{krizhevsky2012imagenet} and textual features by word embeddings \citep{mikolov2013distri}, which we will explain more in \S \ref{subsection:vqa}). We will now introduce two methods of combining multimodal representations: joint and coordinated representations.

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.54\linewidth}
		\includegraphics[width=\linewidth]{joint_reps.pdf}
		\caption{Joint representation.}
		\label{fig:reps1}	
	\end{subfigure}
	\begin{subfigure}[b]{0.43\linewidth}
		\includegraphics[width=\linewidth]{coordinated_reps.pdf}
		\caption{Coordinated representations.}
		\label{fig:reps2}	
	\end{subfigure}
	\caption{Structure of joint and coordinated representations.}
	\label{fig:reps}
\end{figure}

Joint representations project unimodal representations together into a multimodal space. In Figure \ref{fig:reps1} we can see a graph that illustrates the mathematical expression
\begin{equation}
x_m = f(x_1, \dots , x_n)
\end{equation}
for joint representation. We can see how the unimodal representations $x_1, \dots , x_n$ are inserted into the function $f$ that could be e.g.,\ be a deep neural network or a recurrent neural network. This results in the joint multimodal representation $x_m$.
Joint representations are best suited in situations where all the modalities (even more than two) are present during inference. Up until recently most joint representations were a simple concatenation of individual modality features, which is called early fusion \citep{dmello2015review}. More advanced methods are neural networks, graphical models and recurrent neural networks (RNN) \citep{elman1990finding}.

Neural networks are commonly used to combine the modalities images and text \citep{silberer2014learning} or audio \citep{mroueh2015deep, ngiam2011multimodal, wu2014exploring}. They can be trained end-to-end, to learn both representing the data and learning a particular task. Neural network based joint representations have the advantage of being able to pre-train from unlabeled data, if the available labeled data is not sufficient for supervised learning. One disadvantage is, that they do not naturally have the ability to deal with missing data. Variations of probabilistic graphical models on the other hand have the ability to deal with missing data in a natural way. They use latent random variables to construct representations \citep{bengio2013represent} and do not need supervised data for training \citep{salakhutdinov2009boltz}. Both discussed models are only able to represent fixed length data, whereas RNNs and their variants are able to represent varying length sequences. RNNs have been used in tasks such as affect recognition \citep{chen2015multi, nicolaou2011contin} and multimiodal gesture recognition \citep{rajagopalan2016extend}.

Coordinated representations project each modality into a separate but coordinated space. They are coordinated through a similarity or structure constraint (e.g.,\ minimizing cosine distance \citep{frome2013devise}, maximizing correlation \citep{andrew2013deep}, and enforcing a partial order \citep{vendrov2016order} between the resulting spaces). In Figure \ref{fig:reps2} we can see the graphical illustration of the mathematical expression
\begin{equation}
f(x_1) \sim g(x_2)
\end{equation}
for coordinated representations. Here we can see that each modality ($x_1$, $x_2$) has a corresponding projection function ($f$, $g$) which is independently mapped into a coordinated multimodal space, indicated as $\sim$ in the graph. The amount of modalities has been mostly limited to two for coordinated representations. They are suited for applications where only one modality is present at inference time.

% this paragraph can be shortened, if can afford to 'loose' text
There are two subtypes of coordinated representations, namely similarity models and structured models. The former minimize the distance between modalities in the coordinated space. An early example of this is WSABIE (web scale annotation by image embedding) \citep{weston2011wsabie}, where similarity was enforced between image representations and their annotations through a coordinated space. This was attained through higher inner product, which reduced the cosine distance between the corresponding representations. A newer example for coordinated representations is DeViSE (deep visual-semantic embedding) \citep{frome2013devise}, which is based on neural networks. It is similar to WSABIE, but uses more complex image and word embeddings. Another model similar to DeViSE uses videos instead of images \citep{pan2016joint}.
Structured coordinated space models go beyond the enforced similarity between representations and enforce additional constraints between representations. The application usually determines the type of structure that is enforced, with varying constraints for hashing, cross-modal retrieval and image captioning. Cross modal hashing describes the process of compressing high dimensional data into compact binary codes with similar binary codes for similar objects \citep{wang2014hashing}. This can be used for cross-modal retrieval \citep{bronstein2010data, jiang2015class}. Canonical correlation analysis (CCA) \citep{hotelling1936relations} is an example of a structured coordinated space which has been used for cross-modal retrieval \citep{hardoon2004canonical, klein2015associating, Rasiwasia2010ANA} and audiovisual signal analysis \citep{sargin2007audio, slaney2001facesync}. It computes a linear projection which maximizes the correlation between two modalities and enforces orthogonality of the new space.

\subsubsection{Multimodal Fusion Methods}

We previously described early fusion as the simplest form of joint representation. We will now discuss further methods and challenges of multimodal fusion. Multimodal fusion describes the process of joining information from two or more modalities to perform a prediction. There are several issues that might occur, e.g.,\ that information from different modalities have varying predictive power, noise topology, and missing data in at least one of the modalities. However, multimodal fusion methods have many benefits such as allowing more robust prediction, capturing complementary information, and being able to operate, even if one modality is missing. Multimodal fusion can be classified into two main categories: model-agnostic approaches and model-based approaches. The former approach is not directly dependent on a specific machine learning approach and the latter is tied to their construction. In model-agnostic approaches there are different levels of fusion: early fusion \citep{leong2011going, bruni2011distri}, late fusion \citep{gunes2005affect, snoek2005late}, and hybrid fusion \citep{atrey2010hybrid}.

Early fusion or feature level fusion methods create a joint representation of input features from multiple modalities. Once the information is fused, a single model is trained to learn the correlation and interactions between low level features of each modality. The features are commonly concatenated, which is the simplest form of joint representation. The final prediction $p$ is denoted as

\begin{equation}
p = h([v_1, \dots , v_m])
\end{equation}

where $h$ refers to the single model and $ v_1, \dots , v_m$ represent each input modality as a dense vector. The features from different modalities need to be highly engineered and preprocessed, in order for them to align well or share similarities in their semantics. We can infer from the formula that only one model is used to make predictions, assuming that the model is well suited for all the modalities. Early fusion only requires the training of a single model, which makes the training pipeline easier compared to late and hybrid fusion.

Late fusion or decision level fusion, on the other hand, performs multimodal integration at later prediction stages. It uses unimodal decision values and fuses them with a fusion mechanism such as averaging \citep{shutova2016black}, voting schemes \citep{morvant2014vote}, weighting based on channel noise \citep{potamianos2003noise} and single variance \citep{evangelopoulos2013variance}, or a learned model \citep{glodek2011learned, ramirez2011learned}. The final prediction can be denoted as

\begin{equation}
p = F(h_1(v_1), \dots , h_m(v_m))
\end{equation}

where $F$ represents a fusion mechanism and the model $h_i$ is used on modality $ i (i = 1, \dots , M)$. The use of different models on different modalities allows for more flexibility. It makes it easier to handle a missing modality, since the predictions are made separately. However, late fusion ignores the low level interaction between the modalities and is therefore not effective at modeling signal-level interactions between modalities. In this work, we focus on late fusion with co-attention learning that model cross-modal interactions. Examples for such an approach are MCAN \citep{yu2019mcan} and multi-task ViLBERT \citep{lu2020multitask}, which we will discuss in more detail in \S \ref{subsection:mcan} and \S \ref{subsection:vilbert}.

Hybrid fusion exploits the advantages of both feature level and decision level fusion strategies in a common framework. Its successful applications include multimodal speaker identification \citep{wu2005speaker} and multimedia event detection \citep{lan2014multimedia}.

% if we can reduce text/pages, then we could reduce the following paragraphs (up until "Although multimodal...")
Having presented the different fusion methods for model-agnostic approaches, we will now move on to discuss model-based approaches which can be divided into three categories: kernel-based methods, graphical models, and neural networks.

Multiple kernel learning (MKL) methods are an extension to kernel support vector machines (SVM) that are able to use different kernels for different modalities of the data \citep{goenen2011kernel}. Modality-specific kernels in MKL enable better fusion of heterogeneous data, because kernel functions can be seen as similarity functions between data points. They have been an especially popular method for fusing visual descriptors for object detection \citep{bucak2014kernel, gehler2009kernel, krizhevsky2012imagenet} and have also been used for other tasks such as multimodal affect recognition \citep{chen2014recog, jaques2015multi, sikka2013multiple}, multimodal sentiment analysis \citep{poria2015deep}, and multimedia event detection \citep{yeh2012novel}. MKL are flexible in kernel selection and can be used to both perform regression and classification. A big disadvantage of MKL is its reliance on training data during inference time, which leads to slow inference and a large memory footprint.

Graphical models can generally be classified into two main categories: generative (modeling joint probability) and discriminative (modeling conditional probability). Over the years generative models lost popularity to discriminative ones such as conditional random fields (CRF) \citep{lafferty2001crf} and its variations \citep{quattoni2007hcrf, song2012multi, qin2009global}. Graphical models have the advantage of being able to easily exploit spatial and temporal structure of the data. This makes them especially popular for temporal modeling tasks, such as audio-visual speech recognition (AVSR) \citep{gurban2008dynamic} and multimodal affect recognition \citep{baltrusaitis2013dimensional}.

Neural networks have recently become an increasingly popular way to tackle multimodal fusion. Their field of application encompasses fusing information for visual and media question answering \citep{gao2015are, malinowski2015ask, xu2016ask}, gesture recognition \citep{neverova2016moddrop}, affect analysis \citep{kahou2015EmoNets,nojavanasghari2016deep}, and video description generation \citep{jin2016video, venugopalan2016improving}. Shallow \citep{gao2015are} and deep \citep{nojavanasghari2016deep, venugopalan2016improving} neural neural networks have both been explored for multimodal fusion, whereas the advantage of the latter lies in their capacity to learn from a large amount of data. Another advantage of recent neural architectures is their ability for end-to-end training of both the multimodal representation component and the fusion component. In comparison to other non neural network based systems they show good performance and are able to learn complex decision boundaries. However, they also come with disadvantages such as a requiring large training datasets to be successful and a lack of interpretability. 

Although multimodal learning methods have experienced great advances in recent years there are still some challenges that remain. The heterogeneity of data is an issue, which makes it difficult to map data from one modality to another. The relationship between modalities is often open-ended or subjective, making it hard for evaluation if there is no one correct solution or answer. Often times multimodal learning methods can be difficult to interpret, as it might remain unclear what the prediction relies on, and which modalities or features play an important role.

\subsection{Visual Question Answering} \label{subsection:vqa}

The integration of vision and language tasks has significantly advanced multi-disciplinary research from fields of computer vision, natural language processing and deep learning. The underlying challenge of this integration is to find methods that are able to process and relate information from multiple modalities, such as linguistic and visual information, according to the given task. Language and vision integration tasks are very diverse, including visual description generation \citep{plummer2015vdg}, image-text retrieval \citep{wang2016retrieval}, visual commonsense reasoning \citep{zellers2019vcr}, visual entailment \citep{xie2019entailment}, and visual question answering \citep{antol2015vqa}. In our work, we will focus on the visual question answering (VQA) task.

VQA is a challenging multi-modal task and intersects with other vision and language tasks. The goal of VQA is to learn a model to produce a natural language answer for free-form, open-ended natural language questions by reasoning about presented visual content. The visual input can be either images or videos. We will only focus on images as input in the further discussion. In regards to VQA demanding multi-modal knowledge beyond a single domain, it has been widely accepted as an AI-complete task. \citet{geman2015visual} have considered VQA as the Visual Turing Test, where human-level abilities to semantically understand visual information and answering questions are expected. Solving the VQA task efficiently can result in various potential applications. It can for instance help blind users communicate with pictures, allow users of online educational services to interact with images, summarize visual data for surveillance data analysis and lastly through image retrieval improve the search queries on online shopping sites \citep{manmadhan2020vqa}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{vqa_graph.pdf}
	\caption{Given an image and question as input, the visual question answering model predicts an answer to it.}
	\label{fig:vqa-graph}
\end{figure}

Compared to other vision and language tasks, VQA stands out as a particularly challenging task for various reasons. In order to predict an accurate answer, the VQA model requires a fine-grained semantic understanding of both the image and the question. Acquiring this involves solving a wide range of computer vision subtasks such as object recognition, object detection, attribute classification, scene classification, counting, activity recognition, spatial relationships among objects, commonsense reasoning and knowledge-base reasoning \citep{manmadhan2020vqa}. Unlike other tasks, where the question to be answered is fixed and only the image changes, the questions in VQA are not predetermined. Another challenge is the high-dimensionality of the supporting visual information.

%datasets
There are numerous publicly available datasets for validating VQA models with their own characteristics. The first proposed dataset for VQA is DAQUAR (DAtaset for QUestion Answering on Real-world images) \citep{malinowski2014vqa} which contains human question-answer pairs about images. Subsequently, several large scale datasets based on MS COCO (Common Object in Context) \citep{coco} have been proposed. COCO-QA \citet{ren2015exploring} uses COCO image captions to automatically generate questions from them and produces answers of a single-word type. The VQA 1.0 dataset proposed by \citet{antol2015vqa} contains three questions per image and ten ground-truth answers per question. Visual Genome \citep{krishnavisualgenome} represents a more balanced distribution of question types and has larger average question and answer lengths than the VQA 1.0 dataset. Visual7W \citep{zhu2016cvpr} is a part of Visual Genome and adds a 7th `which' question category to accommodate visual answers. A significant deficiency in the previously mentioned VQA datasets is that they are biased. The VQA 2.0 dataset \citep{goyal2017vqa2} attempts to reduce language bias by asking the same question for two images and instructing human annotators to give opposite answers. We will discuss the VQA 2.0 dataset in more detail in \S \ref{subsection:datasets}.
\citet{agrawal12018gvqa} argue that VQA systems are largely driven by superficial correlations in the training data and lack sufficient visual grounding. They found that these VQA systems highly rely on language priors which encourage the systems to blindly output the most common answers by only focusing on the questions without reasoning about the visual content. For example, since about 40\% of questions that begin with ``what sport'' have the answer ``tennis'', systems tend to learn to output ``tennis'' for these questions regardless of image content \citep{wu2019self}. In order to counter the language priors in VQA datasets, \citet{agrawal12018gvqa} propose the VQA-CP dataset where they reorganize the training and validation splits of the VQA 1.0 and VQA 2.0 datasets in a way that the distribution of the question-answer pairs is different to the test set.
\citet{kafle2017tdiuc} developed the dataset TDIUC (Task Directed Image Understanding Challenge) to avoid some of the limitations of previous VQA datasets such as unbalanced question types, questions that can be answered by ignoring images, and difficult evaluation process. Their proposed dataset includes more balanced questions and introduces absurd questions forcing a VQA system to determine if a question is valid for a given image. In addition to that they introduce new evaluation metrics to compensate for biases in VQA datasets. However, the most used evaluation metric for state-of-the-art VQA models is accuracy, which represents the ratio of the number of correctly answered questions to the number of total questions.

Within the last couple years many researchers have proposed different solutions for the VQA task that commonly follow the same structure. The general VQA algorithm can be divided up into three phases: Firstly image featurization and question featurization, secondly joint comprehension, and lastly answer generation.
In the first phase the given image and question are processed independently to obtain separate vector representations. There are multiple ways to extract information about images and questions of which we will only present a selected few.

\subsubsection{Image and Question Featurization}

In the process of image featurization the system needs to extract relevant features of the image to understand the image content. The image feature describes an image as a numerical vector, in order for it to be applied to different mathematical operations. Most VQA models use pre-trained deep neural network models for image featurization, of which convolutional neural network (CNN) \citep{krizhevsky2012imagenet} pre-trained on ImageNet \citep{russakovsky2015imagenet} is the most widely used one due to its good performance. The predominant CNN models trained on ImageNet include AlexNet \citep{krizhevsky2012imagenet}, ZFNet \citep{zeiler2014visual}, VGGNet \citep{simonyan2015very}, GoogleNet \citep{szegedy2015going} and lastly ResNet \citep{residual}.

Plain text or strings cannot be processed by most machine learning algorithms and almost all deep learning architectures. This demands questions to be prepared in a way that can be processed by the system. Word embeddings enable the necessary preprocessing for question featurization. Embeddings can be defined as numerical vectors that represent words or phrases from a vocabulary. These vectors represent a collection of features that hold information about the relation between words. Since word embeddings are trained on word co-occurence, they capture semantic, morphological or contextual information. Different training algorithms and text corpora have an influence on the generated word embeddings. This makes it challenging to choose the best embedding for the VQA task.

Early embedding models have been count based models such as one-hot vector %(a binary vector with exactly one non-zero entry at the position indicating the index of the word in the vocabulary)
and co-occurence matrix \citep{miller1991contextual}. They are simple to implement and interpret, but quickly run into issues concerning their fast growth as the length of the sparse vector is generally the size of the vocabulary. An alternative method for representing a word is to use a short and dense vector. Prediction based models such as CBOW and skip-gram (also called word2vec) \citep{mikolov2013efficient} directly learn word representation and use neural network as their basic component to train a classifier on a binary prediction task. In the last few years hybrid models that combine count based and predict based methods to produce a word embedding have become more popular in NLP research. One prominent example are global vectors (GloVe) \citep{pennington2014glove}, which perform more accurate than skip-gram, because the global corpus statistics are captured directly by the model. An even newer form of embeddings are contextualized word representations such as ELMo \citep{peters2018elmo} and BERT \citep{devlin-etal-2019-bert}.

\begin{figure}[H]
	\centering
	\includegraphics[width=\linewidth]{lstm.pdf}
	\caption{Basic architecture of LSTM question feature extraction.}
	\label{fig:lstm}
\end{figure}

In advanced methods for question feature extraction in VQA neural networks such as convolutional neural network \citep{krizhevsky2012imagenet}, long short term memory (LSTM) \citep{lstm} and gated recurrent unit (GRU) \citep{cho2014gru} are used. The latter two belong to the recurrent neural network (RNN) family \citep{elman1990finding}. \citet{young2018recent} state that sequence based models such as RNN perform better than word sequence independent methods like word2vec. They also claim that LSTM are generally preferred by VQA researchers. It should also be noted that RNN models are not used independently, but are always combined with any previously mentioned embeddings that are passed as input through LSTM or GRU. Figure \ref{fig:lstm} shows the basic architecture of LSTM where word embeddings are the input and question feature is generated as output. Both LSTM and GRU are gating based architectures that are designed to capture long-range dependencies and solve the vanishing gradient problem. The LSTM layer has memory cells where it can store context information e.g.,\ of words in a sequence, when the input is a question. The memory is controlled by gates, of which one gate at each input state decides how much of the new input should be written to the memory cell, and how much of the current content of the memory cell should be forgotten. In Figure \ref{fig:lstm} $h_T$ represents the output state vector from the last time step that is used as a question feature. GRU is an alternative to the LSTM, which has fewer gates and does not have separate memory cells.

\subsubsection{Joint Comprehension} \label{subsubsection:attention}

After the image features and question features are extracted, the features are mapped to a joint space and then combined to generate an answer to the question about an image. There is a wide range of techniques for joint comprehension of image and text of which some were already discussed in \S \ref{subsection:embeds}. We will present a selection of methods for combining multi-modal features that are particularly relevant for the VQA task.

Examining several baseline fusion methods for VQA such as concatenation, element-wise addition and element-wise multiplication, \citet{malinowski2017ask} have found that element-wise multiplication has more accuracy. A more advanced method for VQA is presented by end-to-end deep neural network models. They aim to capture the associations between the modalities better by training specific layers for joint comprehension of image and question features. The composition and use of the layer varies for different models. An interesting example for this method is introduced by \citet{fukui2016multimodal}, which uses a Multimodal Compact Bilinear Pooling (MCB) layer for joint representation of image and question features. They hypothesize that using the outer product of visual and textual vectors is more expressive than simple baseline fusion methods. Another approach to combine multimodal features is based on encoder-decoder architecture. The decoder takes the image and question representations as input and is then trained to generate a correct answer. LSTM networks are commonly used as decoder and can vary in the way they take feature vectors as input. \citet{ren2015exploring} and \citet{zhu2016cvpr} take image encoding as first or last word of question as input to decoder LSTM. \citet{malinowski2017ask} on the other hand take image encoding along with each word of question.

The application of attention mechanism has become widely popular for VQA. It enables VQA to ignore image regions that are irrelevant to the given question and choose to focus on important image regions for predicting the correct answer. In the early stage attention mechanism was mainly used for visual attention, where the focus is on regions of the image. An example of this are Stacked Attention Networks \citep{yang2016vqa} that learn attention on image regions through multiple iterations.

A more advanced method for VQA is co-attention, which not only requires to learn visual attention on the image but also needs to learn textual attention on the question. One such co-attention learning method was proposed by \citet{lu2016hierarchical} that alternately learns image attention and question attention. 
Another co-attention model was introduced by \citet{yu2018beyond} where the learning method is separated into two steps. One is self-attention learning of the question, and the other is question-guided-attention learning of the image. In their approach, multiple attention maps can also be used to improve the capacity of the attended visual representation, where it is fused at a later stage with the attended question representation through multi-modal factorized high-order pooling (MFH).
\citet{Nguyen_2018_CVPR} proposed the dense co-attention network (DCN) which establishes bidirectional interactions between textual and visual modalities by generating an attention map on question words for each image region and vice versa. \citet{ban} introduced bilinear attention networks (BAN), where a bilinear attention map is used to reduce the computational cost to learn attention distribution for every pair of question words and image regions. They also used low-rank bilinear pooling to produce joint question and image representation. However MFH lacks dense interaction modeling between questions and images. DCN and BAN also do not model intra-modal attention. \citet{gao2019dynamic} and \citet{yu2019mcan} proposed new models based on deep co-attention that achieve a better performance on the VQA task. %include vilbert?

\subsection{Grounding}

One of the main challenges of a successful VQA system is to identify and recognize%localize might be better
the most relevant image regions to the question. As discussed in \S \ref{subsubsection:attention}, this is commonly resolved by attention mechanisms. In order to find the regions of the image that lead to the answer, attention mechanisms generate an attention map over the input image. These attention maps are interpreted as groundings of the answer to the most relevant areas of the image \citep{zhang2019interpretable}. Some questions might require external, common sense knowledge to answer them correctly, which can make it impossible for the system to ground their decision in the image that mimic human interpretation. Given an image showing a red fire hydrant and the question `What can the red object on the ground be used for?' the system needs to visually recognize the `red object' as a `fire hydrant', but also to know that `a fire hydrant can be used for fighting fires'. Another instance where VQA systems generally fail is on questions requiring reading \citep{singh2019towards}. Given an image showing a ketchup bottle and the question `What is the brand of this product?' the system needs to visually recognize the `product' as `ketchup bottle', but also to be able read the text on the bottle. If there is more text on the bottle, it also needs to evaluate which text is relevant for grounding the answer to the question.

%Given that some questions may not directly refer to the contents of the image, the system may require external, common sense knowledge to answer them correctly. This can make it impossible for the system to ground their decision in the image in a human interpretable way

To get to one of the underlying issues of this we need to take a closer look at how word embeddings get their meaning. Word embeddings are modeled through distributional semantics where the meaning of a word is entirely constituted by patterns of co-occurrence with other words or other linguistic contexts \citep{baroni2016grounding}. From a cognitive perspective, \citet{barsalou1999perceptual} and \citet{fincher2001perceptual} have argued that language is grounded in physical reality and perceptual experience. \citet{barsalou2008grounded} has coined the term \textit{conceptual grounding} that refers to the idea that language is grounded in perceptual experience and sensorimotor interactions with the environment.

\citet{baroni2016grounding} claims that distributional semantic models do not have access to the sensorimotor world and are therefore affected by the symbol grounding problem \citep{harnad1990symbol}. Grounding cannot be established in distributional semantics, because the meaning of linguistic symbols is given by a distribution over other linguistic symbols, which leads to infinite regress. One way to to disrupt this infinite regress would be to establish an interaction with the world through perceptual and motor properties.

Multimodal models that combine textual and visual modality have the potential to establish that missing link, and allow conceptual grounding. \citet{beinborn2018multimodal} suggested that multimodal concept representations are motivated by the idea that semantic relations between words are grounded in perception. One challenge of conceptual grounding is providing a multimodal representation of abstract concepts due to the lack of perceptual patterns associated with abstract words \citep{hill2014multi}. The occurrence of abstract concepts in questions can lead to issues in the VQA task, as these might not be directly referable to the content of the image. Hence, it is crucial to combine the meaning for concrete and abstract concepts for grounding phrases. The selection of VQA dataset plays a key role in gaining a deeper insight into how successful multimodal models are at conceptual grounding. Many VQA datasets only require relatively shallow image understanding to answer the questions which very likely enables models to get a high accuracy without requiring strong conceptual grounding abilities. Going back to our introductory examples, there are VQA datasets that specifically address issues such as reasoning about questions which require commonsense, or basic factual knowledge \citep{wang2018fvqa}, and reading text in images \citep{singh2019towards}.

%Despite the advantages of multimodal models in capturing semantic relations, \citet{beinborn2018multimodal} question whether they contribute to a cognitively more plausible approximation of human conceptual grounding.
%maybe the last sentence is too negative
%think about potential of grounding, e.g. when one modality is missing (image-retrieval task)

\subsection{Modular Co-Attention Network (MCAN)} \label{subsection:mcan}
%inter-modal, intra-modal and dense interaction modeling
% inspiration of co-attention?
In VQA, extracting discriminative features for textual and image representations is important in order to obtain a fine-grained semantic understanding of both the image and the question. However, using global features extracted from the whole image to represent visual information may introduce noisy information that is irrelevant to the question (e.g.,\ the case where only a small region of the image relates to a question). On the other side, natural language questions may also contain words that are not relevant to the image and therefore can also be considered as noise. These challenges lead to the development of the co-attention learning approach, where the model jointly learn the attentions for both the image and the question simultaneously and allow it to extract more discriminative visual and textual representations. 

%Several methods have been proposed for co-attention learning in VQA. \citet{yu2018beyond} separated the co-attention method into two steps, self-attention for question attention and visual attention conditioned on attended question representation. In their approach, multiple attention maps can also be used to improve the capacity of the attended visual representation, where it is fused at a later stage with the attended question representation through multi-modal factorized high-order pooling (MFH). \citet{Nguyen_2018_CVPR} proposed dense co-attention network (DCN) which establish bi-directional interactions between textual and visual modalities by generating attention map on question words for each image region and vice versa. \citet{ban} introduced bilinear attention networks (BAN), where bilinear attention map is used to reduce the computational cost to learn attention distribution for every pair of question words and image regions. They also used low-rank bilinear pooling to produce joint question and image representation. However MFH lacks dense interaction modeling between questions and images. DCN and BAN also do not model intra-modal attention.

% To address the aforementioned problems, \citet{yu2019mcan} proposed modular co-attention network (MCAN), which simultaneously models dense intra- and inter- modal interactions.
To address the issues with previous methods for co-attention learning discussed in \S \ref{subsubsection:attention}, \citet{yu2019mcan} proposed a modular co-attention network (MCAN), which simultaneously models dense intra- and inter-modal interactions. MCAN is composed of stacked modular co-attention (MCA) layers which consist of the self-attention (SA) and guided-attention (GA) unit based on scaled dot-product attention \citep{transformers}. The SA unit consists of a multi-head attention layer and a feed-forward neural network (FFNN) layer. It accepts a group of input features $X = [x_{1}, \dots, x_{m}]$, where $m$ is the number of features, and passes it through the multi-head attention module to learn the pairwise relationship between every possible pairing $\langle x_{i},x_{j} \rangle$ in $X$ to obtain the attended output features $Z$ using weighted summation of $X$. The output of the multi-head attention layer $Z$ is then fed through two fully-connected layers with a ReLU activation \citep{relu} and dropout units \citep{dropout}. A residual connection \citep{residual} is employed around each of the two layers, followed by layer normalization \citep{ba2016layer} to improve optimization.

The GA unit is almost similar to the SA unit. It is composed of a multi-head attention and a FFNN layer. The GA unit accepts two groups of input features $X$ and $Y = [y_{1}, \dots, y_{n}]$, where $n$ is the number of features for another modality (i.e. if $X$ is a question embedding, $Y$ should be an image embedding and vice versa). The GA unit learns to model the pairwise relationship between every possible pairing $\langle x_{i},y_{j} \rangle$ from $X$ and $Y$. The output $Z$ for the GA unit can be understood as attended features for $X$ guided by $Y$. In order to guide the attention learning, for the GA unit key and value matrices for the multi-head attention are computed with respect to $Y$, while queries are computed with respect to $X$.

In a sense, the attended output feature $z_{i} \in Z$ can be seen as a reconstruction of $x_{i} \in X$: 1) by all $x \in X$ with respect to their normalized intra-modal similarities to $x_{i}$ for the SA unit and 2) by all $y \in Y$ with respect to their normalized cross-modal similarity to $x_{i}$ for the GA unit. Both SA and GA unit can be modularly combined to obtain various configurations. For instance, 2 SA units can be used to model the dense intra-modal interaction between each question word pair $Y$ and each image region pair $X$ separately. Afterwards, the attended visual and question features are fed to a GA unit to model dense inter-modal interactions between each word with each image region. We refer to this configuration as SA(Y) - SGA(X,Y) in the paper.

In this model, a single-layer LSTM encoder \citep{lstm} is used to compute question representations from GloVe embeddings \citep{pennington2014glove}. Instead of using only the last hidden state of the LSTM encoder, MCAN utilizes the hidden states for all time steps as question embedding, yielding a feature matrix $I \in \mathbb{R}^{n \times d_{i}}$ where $n$ is the number of words in the question and $d_{i}$ is the size of the LSTM hidden units. For image representation, a set of regional visual features in a bottom-up manner \citep{Anderson_2018_CVPR} is extracted from a Faster R-CNN \citep{faster_rcnn} with a ResNet-101 backbone \citep{residual}. Each $k$-th object is represented as $d_{j}$-dimensional feature vector by mean-pooling the convolutional feature from its detected region, resulting in an image feature matrix $J \in \mathbb{R}^{m \times d_{j}}$, where $m$ is the number of detected objects.

There are two variants of deep co-attention models using the MCA layer: \textit{stacking} and \textit{encoder-decoder}. In the \textit{stacking} model, $L$ MCA layers are stacked in depth with $Z_{I}^{(L)}$ and $Z_{Q}^{(L)}$ as the final attended image and question features respectively. The input features are passed recursively formulated as follows:
\begin{align}
    [X^{(l)}, Y^{(l)}] = \text{MCA}^{(l)}([X^{(l-1)}, Y^{(l-1)}])
\end{align}

On the other side, the \textit{encoder-decoder} model is derived from the Transformer architecture \citep{transformers}. Instead of using $Y^{(l)}$ as input features to guide the attention learning of the GA unit for each $l$-th layer of MCA, only the question features from the last MCA layer $Y^{(L)}$ are utilized. This can be seen as an encoder learning the attended question features $Y^{(L)}$ and a decoder learning the attended image features $X^{(L)}$ conditioned on $Y^{(L)}$. The computation for the \textit{encoder-decoder} model is as follows:
\begin{align}
    Y^{(l)} &= \text{SA}^{(l)}(Y^{(l-1)}) \\
    X^{(l)} &= \text{SGA}^{(l)}([X^{(l-1)}, Y^{(L)}])
\end{align}
using SA(Y) - SGA(X,Y) configuration as an example. In both variants, input features $X^{(0)}$ and $Y^{(0)} $ are set to $X$ and $Y$, respectively.

In the last phase, an attentional reduction model with a two-layer FFNN with a ReLU activation and dropout units is used to obtain the final attended features $\Tilde{x}$ and $\Tilde{y}$ separately. Both $\Tilde{x}$ and $\Tilde{y}$ are obtained as the following:
\begin{align}
    \alpha^{x} &= \text{softmax}(\text{FFNN}_{x}(X^{(L)})) \\
    \alpha^{y} &= \text{softmax}(\text{FFNN}_{y}(Y^{(L)})) \\
    \Tilde{x} &= \sum_{i=1}^{m}\alpha^{x}_{i}x^{(L)}_{i}  \label{eq_mcan_att} \\ 
     \Tilde{y} &= \sum_{i=1}^{n}\alpha^{y}_{i}y^{(L)}_{i}
\end{align}
where $\alpha^{x}$ and $\alpha^{y}$ are the learned attention weights for $X$ and $Y$ respectively. The multimodal fusion feature is then computed as element-wise sum:
\begin{align}
    z = \text{LayerNorm}(W^{\top}_{x}\Tilde{x} + W^{\top}_{y}\Tilde{y})
\end{align}
where $W_{x}, W_{y} \in \mathbb{R}^{d \times d_{z}}$ are two transformation matrices and $d_{z}$ is the dimension of the fused feature. The fused feature $z$ can then be used to make predictions by projecting it to a vector $s \in \mathbb{R}^{N}$ followed by a sigmoid activation, where $N$ is the number of the most frequent answers obtained from the training set.

\subsection{Vision-and-Language BERT (ViLBERT)} \label{subsection:vilbert}
% also discuss uniter, vl-bert, lxmert, etc? change section name?
Recently, BERT \citep{devlin-etal-2019-bert} has achieved state-of-the-art results on a wide array of NLP tasks, such as question answering, natural language inference, and named entity recognition, without any substantial task-specific architecture modifications. \citet{lu2019vilbert} proposed Vision-and-Language BERT (ViLBERT), which is an extension of BERT to jointly learn task-agnostic visual grounding and reason about text and images. 

In ViLBERT, two parallel BERT-style architectures are used to model intra-modal interactions and fuse them through attention-based cross modal interactions. Besides standard transformer blocks \citep{transformers}, a novel co-attentional transformer layer is introduced to facilitate information exchange between different modalities. The co-attentional transformer layer enables simultaneous attention learning by computing key and value matrices from each modality and passing them as input to the multi-head attention layer of the other modality. As a consequence, the multi-head attention layer produces attended image features conditioned on language and attended language features conditioned on images. Both attended features are then fed through a FFNN according to their respective stream with a residual connection with their initial representations, followed by layer normalization (similar to the standard transformer layer). The model itself is composed of alternating transformer blocks and co-attentional transformer layers stacked in series. 

ViLBERT takes as input an image $I$ and text segment $W$ where both of them are represented as the sequence $\{\mathtt{IMG}, v_{1}, \dots, v_{\tau}, \mathtt{CLS}, w_{1}, \dots, w_{\tau}, \mathtt{SEP}\}$ where $v_{i}, 1 \leq i \leq \tau$ are set of region features and $w_{i}, 1 \leq i \leq \tau$ are word tokens and the $\mathtt{IMG, CLS, SEP}$ tokens are special markers. Afterwards, the model outputs final embeddings for each input token $\{h_{\mathtt{IMG}}, h_{v1}, \dots, h_{v \tau}, h_{\mathtt{CLS}}, h_{w1}, \dots, h_{w \tau}, h_{\mathtt{SEP}}\}$. $h_{\mathtt{IMG}}$ and $h_{\mathtt{CLS}}$ correspond to mean-pooled features that represent the entire image and text segment which can be used for downstream language and vision tasks.

For pre-training tasks, ViLBERT is trained with masked multi-modal modeling and multi-modal alignment prediction on the Conceptual Captions \citep{sharma-etal-2018-conceptual} dataset. The masked multi-modal modeling task is derived from the masked language modeling task in BERT \citep{devlin-etal-2019-bert}. Both words and image region inputs are masked approximately 15\% at random and the model is tasked to reconstruct the words and image region given the unmasked inputs. For masked image regions, the features are set to zero 90\% of the time and unchanged 10\% of the time. Masked words are replaced with a $[\mathtt{MASK}]$ token 80\% of the time, a random token 10\% of the time and unaltered 10\% of the time. 

In order to predict the masked values, a distribution over semantic classes for the corresponding image region is set as target as opposed to directly regressing the values. As supervision, the output distribution for the region from a pre-trained detection model for image feature extraction is used. The model is then trained to minimize the Kullback-Leibler divergence between these two distributions.

The objective in multi-modal alignment prediction is to present the model with image and text pairs and the model must predict whether, if the text describes the image. In order to do this, an element-wise product between $h_{\mathtt{IMG}}$ and $h_{\mathtt{CLS}}$ is computed as image-text representation. A linear layer is then added on top of the representation with a sigmoid activation to predict if the image and text are aligned or not. For negative samples, either the image or the text is replaced with another one randomly sampled from the dataset. Similar to BERT, ViLBERT can be fine-tuned on downstream tasks by adding a FFNN layer on top of the final representations and train it in an end-to-end manner.

\citet{lu2020multitask} improved ViLBERT further with multi-task learning, resulting in a single unified model that can perform impressively on many language and vision tasks such as visual question answering, caption-based image retrieval, grounding referring expressions and visual entailment. Multi-task ViLBERT considers 4 groups of tasks to train on --- vocab-based VQA (VQA 2.0 \citep{goyal2017vqa2}, GQA \citep{hudson2019gqa}, and Visual Genome QA \citep{krishnavisualgenome}), image retrieval (MS-COCO \citep{coco} and Flickr30K \citep{plummer2015vdg}), referring expressions (RefCOCO(+/g) \citep{kazemzadeh-etal-2014-referitgame, maorefcoco}, Visual7W \citep{zhu2016cvpr}, and GuessWhat \citep{guesswhat_game}), and multi-modal verification ($\text{NVLR}^2$ \citep{suhr-etal-2019-corpus} and  SNLI-VE \citep{xie2018visual}). Multi-modal verification is the task where given one or more images and a natural language sentence, the model should determine the truthness or predict the semantic relationship between the sentence and the image. 

For each task, a task-specific layer is added on top of a ViLBERT model with shared parameters across all tasks. In order to accommodate this, a new task token $\mathtt{TASK}_{t}$ is also added such that the input to ViLBERT is $\{\mathtt{IMG}, v_{1}, \dots, v_{\tau}, \mathtt{CLS}, \mathtt{TASK}_{t}, w_{1}, \dots, w_{\tau}, \mathtt{SEP}\}$. Vocab-based VQA is treated as multi-label classification, where the Hadamard product between $h_{\mathtt{IMG}}$ and $h_{\mathtt{CLS}}$ is computed and passed to a two-layer FFNN with a sigmoid activation as follows:
\begin{align}
    P_{v}(A|I, Q) = \sigma(\text{FFNN}(h_{\mathtt{IMG}} \odot h_{\mathtt{CLS}}))
\end{align}
where $A$ is the answer, $I$ is the image and $Q$ is the question. For image retrieval, the task-specific layer is trained in a 4-way multiple-choice setting against hard-negatives. An alignment score is computed between image-caption pairs formulated as:
\begin{align}
    \mathtt{Rel}(I,Q) = W_{i}(h_{\mathtt{IMG}} \odot h_{\mathtt{CLS}})
\end{align}
where $W_{i} \in \mathbb{R}^{d \times 1}$ is followed with a softmax activation. The referring expressions task is viewed as a reranking of a set of image region proposals given the referring expression. The final representation $h_{v_{i}}$ for each image region $i$ is passed through a projection layer $W_{r} \in \mathbb{R}^{d \times 1}$ to learn a matching score between the proposed region and the matching score: 
\begin{align}
    \mathtt{Rel}(v_{i}, Q) = W_{r}h_{vi}
\end{align}
where in this case $Q$ can be either a phrase, question or dialog depending on the datasets (RefCOCO(+/g), Visual7W, and GuessWhat). 

Lastly, 2 different task-specific heads are defined for multi-modal verification due to a different task structure between $\text{NVLR}^2$ and SNLI-VE. In the case of $\text{NVLR}^{2}$, the model must determine the truthness of the statement given the images, which is framed as a classification problem. Given an embedding that encodes two image-statement pairs $(I_{0}, Q)$ and $(I_{1}, Q)$, the truthness probability is computed with a 2-layer FFNN as follows:
\begin{align}
    P_{v}(C|I_{0}, I_{1}, Q) = \text{softmax}\left(\text{FFNN}\left(\begin{bmatrix} 
    h_{\mathtt{IMG}}^{0} \odot h_{\mathtt{CLS}}^{0} \\
    h_{\mathtt{IMG}}^{1} \odot h_{\mathtt{CLS}}^{1}
    \end{bmatrix}\right)\right)
\end{align}
where $\mathbb{[}\: \mathbb{]}$ is concatenation. As for SNLI-VE, the input is an image premise and a text hypothesis, and the model must predict the relation between the image and the statement (entailment, neutral, contradiction). Using a linear layer, the element-wise product between $h_{\mathtt{IMG}}$ and $h_{\mathtt{CLS}}$ is then mapped to one of the three labels.

%pre-training objectives, image feature 12-in-1


\section{Approach}
%also explain rationale why to choose both models and differences between them
%vilbert co-attentional transformer layers as SA-SA GA-GA, mcan also can be arranged as enc-dec,
%SGA-SGA lower for MCAN
%CWR vs glove, add a bit about bert
%modification of fusion
\subsection{MCAN vs ViLBERT}
Despite being based on a transformer architecture, both MCAN and ViLBERT have several key differences. First of all, in MCAN the information exchange between modalities is unidirectional. Only one modality can be used to guide another modality at a time (e.g.,\ either attended image feature learning guided by question or attended question feature learning guided by image). On the other side, the novel co-attentional layer in ViLBERT allows bidirectional information exchange between language and vision at varying representation depths. Both architectures also differ in how textual and visual representations are fused. MCAN learns to project textual and image representations to a shared space and sum them together, while ViLBERT applies element-wise product between $h_{\mathtt{IMG}}$ and $h_{\mathtt{CLS}}$ as joint representation of the visual and linguistic inputs. 

Interestingly, it turns out that the co-attention mechanism used by ViLBERT can be considered as a special composition of the modular SA and GA units, as depicted in Figure \ref{fig:vilbert_coattention}.

\begin{figure}[ht]
	\centering
	\begin{subfigure}[b]{\textwidth}
	\centering
	\includegraphics[scale=0.7]{attention}
	\caption{MCAN with SGA(X,Y) - SGA(Y,X) layer.}
	\end{subfigure}
	\begin{subfigure}[b]{\textwidth}
	\centering
	\includegraphics[scale=0.7]{vilbert_coattention}
	\caption{ViLBERT co-attentional transformer layer.}
	\end{subfigure}
\caption{Comparison of ViLBERT against MCAN with SGA(X,Y) - SGA(Y,X) layer. Note the similarities of information flows between the two models.}
\label{fig:vilbert_coattention}
\end{figure}

Here $H_{v}^{(i)}$ and $H_{w}^{(j)}$ are obtained from standard transformer layers. It is important to note that multi-task ViLBERT as SGA(X,Y) - SGA(Y,X) is trained with different training objectives and additional tokens which yield different visual and textual representations compared to the MCAN model. 

Joint representations play an important role in visual question answering and many other language and vision tasks, since they relate important information between visual and textual modalities. As such, comparing joint representations derived from a single unified model (multi-task ViLBERT) against a task-specific model (MCAN), which share similar modular units, might reveal the importance of different training objectives and information flows in generating highly effective multimodal representations.

%this facts make mcan and vilbert comparable, if we thought vilbert as sga-sga with different training objective and additional tokens, also makes it interesting to compare joint representations derived from a single model compared to ones learned from task-specific model.

\subsection{Applying BERT to MCAN}
As opposed to ViLBERT's linguistic stream that is initialized with BERT, the original MCAN model uses GloVe as word embeddings, which is non-contextual and trained on global word-word co-occurrence statistics. However, for VQA, context also provides additional cues which may be helpful for the model to predict the answer accurately and enrich the information in multimodal embeddings (e.g.,\ distinguishing ``bank'' as a financial institution and as land besides water provides extra information for the model when answering questions, especially when both appear together in a question). 

Works such as ELMo \citep{peters2018elmo} and BERT \citep{devlin-etal-2019-bert} assign each word a vector as a function of the entire input sequence, which also enables them to model the use of words in a contextual manner. For our experiments, we use contextual word representations from BERT. BERT is a bidirectional language representation model trained jointly with a masked language model and next sentence prediction objective on BooksCorpus \citep{bookscorpus} and English Wikipedia, with an approximate total of 3,300M tokens. We use BERT-base (12-layer transformers, 768-hidden) and BERT-large (24-layer transformers, 1024-hidden). This brings both MCAN and ViLBERT to an equal footing with respect to their word representations.

We apply BERT with two variants: 1) as contextualized word embeddings to replace GloVe and 2) as question encoder, by replacing the entire LSTM-based encoder (with GloVe embeddings as input) in the MCAN model with the BERT transformer. We excluded the $\mathtt{CLS}$ token when using BERT both as embedding and as encoder, as the $\mathtt{CLS}$ token is mainly included when BERT is used in the fine-tuning manner. For co-attention learning, we use the \textit{encoder-decoder} model with SA(Y) - SGA(X,Y) layers as it performed better compared to the \textit{stacking} strategy \citep{yu2019mcan}. The architecture for both variants is depicted in Figure \ref{fig:mcan_bert}.

\begin{figure}[ht]
	\centering
	\includegraphics[scale=0.7]{enc-dec_mcan_bert}
	\caption{Encoder-decoder MCAN (SA(Y) - SGA(X,Y)) with BERT. We apply BERT as contextualized word representations and as question encoder, replacing a single-layer LSTM.}
	\label{fig:mcan_bert}
\end{figure}
% use BERT as features and encoder, exclude BERT-large due to computational constraint.
\section{Experimental Setup}
% compare quality of multimodal representations by their performance on vqa and gqa
% compare question grounding, as proxy to evaluate quality of multimodal representations
% compare fusion method(?)
% maybe add one sentence how we investigate the quality of jmr
We aim to investigate the characteristics of joint multimodal representations derived from task-specific model and single, unified model for various language and vision tasks. Furthermore, we also want to answer to what extent the models ground their reasoning in the image, rather than making guesses by memorizing question prior from the training data. Our experiments, which we describe below, are intended to compare the joint multimodal representations and how different training objectives and information flows affect them.

%how the quality of joint multimodal representations are affected by different training objectives and information flows. 

\subsection{Datasets} \label{subsection:datasets}
%VQA, image from MSCoco, GQA: statistics for training, included annotation

The VQA 2.0 dataset \citep{goyal2017vqa2} was initially developed to balance the existing VQA 1.0 dataset \citet{antol2015vqa} by collecting complementary images (CI) and corresponding new answers. It is almost double the size of the VQA 1.0 dataset containing approximately 1.1M question-image pairs (QI) with approximately 13M associated answers on the approximately 200K images from MS COCO \citep{coco}. The question-answer pairs are annotated by humans, with three questions per image and 10 answers per question. The VQA 2.0 dataset is split into three: train (195K CI, 443K QI), val (93K CI, 214K QI), and test (191K CI, 453K QI). The test set is further split into four: test-dev, test-standard, test-challenge and test-reserve. The evaluation metric computes the accuracies for three different answer types (yes/no, number, and other) and overall accuracy.

GQA is a dataset \citet{hudson2019gqa} for visual reasoning and compositional question answering over real-world images. It consists of 113K images and 22M questions of assorted types and varying compositionality degrees. Each image is annotated with a scene graph of objects and relations. The images are from MS COCO \citep{coco} and Flickr \citep{thomee2016flickr}, the image scene graphs are based on Visual Genome \citep{krishnavisualgenome}. Each question is associated with a structured representation of its semantics and has a higher average length compared to the questions in the VQA 2.0 dataset. Each answer is complemented with both textual and visual justifications, pointing to the relevant region within the image. In addition to the standard accuracy metric and the more detailed type-based diagnosis, the GQA dataset includes five new metrics. The first one being consistency, which measures the responses consistency across different questions. The validity metric checks whether a given answer is in the scope of the question. The plausibility score measures whether the answer is reasonable, or makes sense, given the question. The distribution metric measures the overall match between the true answer distribution and the model predicted distribution. And lastly the grounding score, which checks whether the model attends to regions within the image that are relevant to the question.

\subsection{Implementation and Hyperparameters}
%hyperparam for mcan
%batchsize, dropout, optimizer, learning rate, decay rate, warmup, exclude bert-large encoder, epoch, layer size, vocab size, split
%uncased bert for all experiments, concatenation
%image feature extraction, padding, word truncation
% for vilbert we use trained model due to high computational cost
We extend the original MCAN implementation in OpenVQA\footnote{\url{https://github.com/MILVLG/openvqa}} and multi-task ViLBERT\footnote{\url{https://github.com/facebookresearch/vilbert-multi-task}}. We also use the HuggingFace BERT implementation \citep{wolf2019huggingfaces}\footnote{\url{https://github.com/huggingface/transformers}}. When utilizing BERT as features, we freeze the weights and concatenate the four last hidden layers of BERT-base and BERT-large as opposed to the last layer, as it yields the best performance. BERT as encoder is fine-tuned in an end-to-end manner following \citep{devlin-etal-2019-bert}. Our MCAN model accepts input image features and input question features of size $d_{x}$ = 2,048 and $d_{y}$ = 512 respectively and produces fused multi-modal features with the dimensionality $d_{z}$ = 1,536. Due to the dimensionality of BERT hidden state, we set $d_{y}$ = 768 when applying BERT-base as encoder. We did not use BERT-large as encoder due to high computational cost. We use uncased variants of BERT in all of our experiments.

VQA image features are extracted from Faster R-CNN (with ResNet-101 backbone) while the question words are tokenized to wordpiece tokens of 14 tokens maximum following the original MCAN implementation. We use object-based features for GQA image representation, which can be downloaded from the official website. GQA questions are truncated to a maximum of 29 tokens. Due to the dynamic number of image regions $m$ and question length $n$, we use zero-padding to fill the image feature matrix $J$ and question feature matrix $I$, where $m_{\mathtt{VQA},\mathtt{GQA}}$ = 100, $n_{\mathtt{VQA}}$ = 14, $n_{\mathtt{GQA}}$ = 29. For MCAN, the number of attention heads is set to 8 and the number of layers is set to 6. The size of the answer vocabulary is set to $N$ = 3,129 for VQA following \citet{yu2019mcan} and $N$ = 2,933 for GQA. For optimization, we use Adam \citep{kingma2014adam} with $\beta_{1}$ = 0.9 and $\beta_{2}$ = 0.98. The learning rate is set to $\min( (lr/(w+1)) * t, lr)$ where $t$ is the current epoch starting from 1, learning rate $lr = 7e^{-5}$ and warmup epoch $w$ = 3 for VQA and $w$ = 2 for GQA. We also use linear decay during training, where the learning rate is decayed by 1/5 every 2 epochs. The decay is applied for VQA after the 10th epoch and for GQA after the 8th epoch. MCAN models with BERT are trained up to 11 epochs for GQA and up to 13 epochs for VQA with batch size 64. For VQA training, we use both $train$ and $val$ splits with additional VQA samples from Visual Genome, while for GQA, both balanced $train$ and $val$ splits are used.

Again, due to computational constraint, we use the pre-trained 6-layer ViLBERT model trained with 12 datasets. Image features for ViLBERT are extracted from a ResNeXT-152 \citep{Xie2016} Faster R-CNN model trained on Visual Genome with attribute loss, resulting in a maximum of 100 image regions. During VQA training, the question words are tokenized into wordpiece tokens and trimmed to a maximum of 23 tokens. For GQA, the question words are truncated to a maximum of 26 tokens. The truncation difference between MCAN and multi-task ViLBERT occurs due to the cleaned split used for multi-task training, where test images are removed from \textit{train} or \textit{val} split for all tasks.

%\subsection{Baseline Model}
%mcan small enc-dec

\subsection{Grounding}
%how we get attention for MCAN and ViLBERT
%how grounding scores are calculated
We want to understand to what extent the joint multimodal representations induced by VQA models relate visual and textual information. Do the models learn superficial correlations in the training data or do they base their reasoning on the images? In order to answer this question, we evaluate question grounding as a proxy to compare the quality of joint multimodal representations. We focus on the GQA dataset, as it offers grounding annotation in the form of a visual pointer that points to regions in the image which the question refers to and are relevant to answer it. 

The question grounding metric is calculated as follows. For each question-image pair $(q, i)$ exists one or multiple pointers $r$ that point to relevant image regions when answering the question. The model visual attention over the image $i$, which is represented by object-based features in our experiments, is then intersected with pointer/s $r$ to obtain the intersection rate. Afterwards, the intersection rate for each bounding box is multiplied with its attention score and then summed up to obtain the overall attention over $r$. This yields the grounding score for an instance of $(q, i)$. The overall grounding score is then computed by averaging the grounding score over all questions in the dataset. GQA also includes spatial-based features for training and evaluation, which we did not have the opportunity to explore due to computational issues.

For MCAN, we use the attention weights obtained from the visual attentional reduction module, with $\alpha = [\alpha_{1}, \alpha_{2}, \dots, \alpha_{m}]$ for $m$ bounding boxes. As for multi-task ViLBERT, we take the self-attention distribution from the last layer for the $h_{\mathtt{IMG}}$ token, as it is the holistic representation of the image. This is intended to get the most accurate attention scores over the image region, as ViLBERT does not have an attention reduction module. We also examine the grounding scores for all 8 attention heads to measure how actively different heads pay attention to the image regions. We then take the average of all attention heads as final grounding score for ViLBERT. We normalize all bounding boxes for MCAN and multi-task ViLBERT.

We compare the grounding scores for MCAN against multi-task ViLBERT, as it measures the grounding capability that a task-specific model can achieve as opposed to a single model for various language and vision tasks. Furthermore, it also shows how unidirectional or bidirectional information streams for co-attention learning affect the robustness of the joint multimodal representation.

% bounding boxes are normalized


\subsection{Fusion Method}
While it is not possible to provide a one-to-one comparison between joint representations in MCAN and multi-task ViLBERT due to differences in architecture and learning strategy, we intend to compare both representations as fair as possible. Therefore, we focus on the cross-modal feature fusion method. MCAN learns to project textual and image representations to a shared space and then performs an element-wise sum on them. On the other hand, ViLBERT applies an element-wise product on $h_{\mathtt{IMG}}$ and $\mathtt{CLS}$ tokens to obtain joint representations. We modify the late fusion method for MCAN as the following:
\begin{align}
    z &= \text{LayerNorm}(W^{\top}_{x}\Tilde{x} \odot W^{\top}_{y}\Tilde{y}) \\
\end{align}
and for ViLBERT:
\begin{align}
    z &= h_{\mathtt{IMG}} + h_{\mathtt{CLS}}
\end{align}
where $z$ is the fused representation. Although we use the pre-trained multi-task ViLBERT that is not trained with an element-wise sum, we want to investigate the performance of the modified fusion method in visual question answering and compare it with the original fusion method. 

\section{Results and Discussion}
%vqa result, gqa result, grounding (possibly with all attention heads with vilbert), ablation of fusion method, transfer performance, attention visualization(?), qualitative analysis of cases where model fails?
% task that appear in other v&l tasks but not vqa, learned by vilbert and allow it to generalize better?
% why encoder is better (?) relate with coref
% explain why vilbert is better than mcan in general, relate to research question
% vilbert may delegates the grounding on several attention heads?
\subsection{Comparison on VQA 2.0} \label{comp_vqa}
Table \ref{table:vqa_result} summarizes our results for different configurations of MCAN models in comparison to multi-task ViLBERT. The baseline MCAN-small model corresponds to the one described in \citet{yu2019mcan} using GloVe word embeddings passed through a one-layer LSTM network with a hidden size of 512. We observe that the multi-task ViLBERT model significantly outperforms various MCAN models in all metrics.

One possible advantage of ViLBERT over our MCAN models is the bidirectional visual-linguistic stream, which enables simultaneous information exchange between different modalities. Coupled with a multi-task training objective on a plethora of datasets which are divided into 4 task groups, ViLBERT is also able to learn better associations between language and visual concepts due to parameter sharing in both base ViLBERT model and branching task-specific heads, therefore resulting in higher accuracy. \citet{lu2020multitask} observed that multi-task training has a regularizing effect on tasks which overfit when trained separately. This explains its benefit over the task-specific MCAN models. We find that multi-task ViLBERT is especially better compared to our best-performing MCAN model in counting by 3.42 points. This might be due to ViLBERT being trained on several VQA datasets in a round-robin fashion, which improves the model performance in counting. It is important to note that we do not apply BERT to MCAN with \textit{stacking} SGA(X, Y) - SGA(Y, X), which is similar to the ViLBERT architecture, due to computational constraint. \citet{yu2019mcan} also reported that such a configuration did not offer comparative performance to SA(Y) - SGA(X, Y). Thus, we presume that the reason ViLBERT performs better is mainly due to multi-task training.

Out of all MCAN models, we observe that MCAN with BERT-base encoder performs the best in all categories, surpassing the baseline model by 0.4 points in accuracy. Applying BERT as encoder has also achieved strong performance in tasks such as coreference resolution \citep{joshi-etal-2019-bert}. However, replacing GloVe with BERT as contextual word representations only yields a marginal performance increase in general, which is surprising considering how BERT significantly improves the performance in many downstream NLP tasks. The sole exception where BERT performs worse compared to GloVe is when we use BERT-large as embeddings. We attribute this to our decision in choosing the hidden state dimension of the LSTM encoder similar to the baseline model, which might be too low to model the question representation. An indication for this is that the model still does not converge, even after training it with more epochs. 

We hypothesize that the performance difference when using BERT as encoder and as features is mainly due to the LSTM encoder, which is similar to the baseline model. Based on the results, we conclude that the LSTM encoder, which is unidirectional, prevents the model from making full use of the contextual information from BERT embeddings. Using a bidirectional LSTM has the potential to improve the performance further, as it might be able to incorporate the contextual information from BERT embeddings better. Another possible cause for this is that we use a single layer LSTM, which might be too shallow to properly model the question representation. Using a deeper LSTM network might be beneficial for better question modeling.

% Comparing our best performing model MCAN with BERT-base encoder to both MCAN with BERT-base and BERT-large features, the main difference might be the suitability of the encoder for BERT embeddings. Based on the results we could conclude that BERT-base embeddings might not be as well suited for a LSTM encoder, as they are bidirectional. The LSTM we used based on the baseline model is unidirectional, which might prevent the model from making full use of the contextual information from the BERT embeddings. Using a bidirectional LSTM has the potential to improve the performance, as it might be able to better incorporate the contextual information from the BERT embeddings. Another issue with the architecture of the LSTM might be that it is only one layer deep. A deeper LSTM network might be beneficial for using BERT embeddings.

% yes/no questions are easier than other questions. so higher score is expected

\begin{table}[ht]
\captionsetup{singlelinecheck = false, justification=justified}
\setlength\tabcolsep{0pt} % let LaTeX figure out amount of inter-column whitespace
\label{turns}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} l *{4}{d{2.4}} }
\toprule
 \multicolumn{1}{l}{Model} & \multicolumn{1}{c}{Overall} & \multicolumn{1}{c}{Y/N} & \multicolumn{1}{c}{Num} & \multicolumn{1}{c}{Other}\\
\midrule
\midrule
MCAN-small (baseline) & 70.63 & 86.82 & 53.26 & 60.72 \\
MCAN /w BERT-base features & 70.70 & 86.82 & 52.94 & 60.96 \\
MCAN /w BERT-large features & 70.53 & 86.51 & 53.01 & 60.86  \\
MCAN /w BERT-base encoder & 71.03 & 87.34 & 53.30 & 61.12 \\
\midrule
Multi-task ViLBERT & 72.57\ast & 88.33\ast & 56.72\ast & 62.68\ast \\
\bottomrule
\end{tabular*}
\caption{Results for MCAN models and multi-task ViLBERT on the \textit{testdev} split of the VQA 2.0 dataset \citep{goyal2017vqa2}. Models are evaluated for overall accuracy as well as the accuracies for three different answer types (yes/no, number, and other). Asterisk denotes the best performance on each metric.}
\label{table:vqa_result}
\end{table}

\subsection{Comparison on GQA}
% gqa is also more challenging compared to vqa. it is possible that using same learning rate for bert encoder 
% cause catastrophic forgetting
% what is used to calculate distribution score?
% difficulty of GQA highlight vilbert superior performance -> gap is bigger compared to vqa
% relate to research question
%easier to converge in gqa compared to vqa?
We compare the results for MCAN models and multi-task ViLBERT on the GQA dataset \citep{hudson2019gqa}, which is depicted in Table \ref{table:gqa_result}. Again, we examine that multi-task ViLBERT significantly outperforms the MCAN models in all metrics. Compared to the previous results on VQA 2.0, the gap between the performance of multi-task ViLBERT and MCAN models is substantially larger (by 5.62 points for accuracy on average).

The superior results of ViLBERT could be attributed to its bidirectional information stream and robust generalization across language and vision tasks which is achieved by multi-task training, similar to its performance on the VQA 2.0 dataset. We observe that multi-task ViLBERT achieves the best distribution score (for this metric, lower is better), which demonstrates that the model is able to predict not only the most common answers, but also the less frequent ones. This can be explained by the fact that multi-task training allows for better generalization compared to task-specific training, which in return enables ViLBERT to learn more robust representation. In comparison with the similar metric in VQA 2.0 (Y/N), multi-task ViLBERT also shows superior performance in the binary metric with a gap of 5.53 points over our best MCAN model.

% The better distribution score (for this metric, lower is better) of multi-task ViLBERT shows that the model not only predicts the most common answers but also the less frequent ones. The reason for this better performance might be that multi-task ViLBERT has been pre-trained on a high count of datasets and categories. This big pool of information might allow the model to better indicate more subtle trends in the dataset's distribution. The result of multi-task ViLBERT for binary answer types reflects a similar superior performance to MCAN compared to the yes/no answer type on the VQA 2.0 dataset.

% similar to vqa, bert offer marginal performance increase
%while the baseline MCAN model performs the lowest in all categories. %(why is the baseline model best at distribution?)
Similar to the result in VQA, MCAN with BERT only offers a marginal improvement in performance across all metrics. We examine that out of all MCAN models, MCAN with BERT-base features scores the highest on overall accuracy and binary. Interestingly, MCAN with BERT-base encoder performs the worst out of our modified MCAN models, contrary to our previous VQA 2.0 results. We attribute this to our decision in using the same learning rate for MCAN and BERT encoder. We suspect this introduces catastrophic forgetting, which damages the learned language representation in BERT due to overly aggressive fine-tuning. We also did not observe this phenomenon when training MCAN with BERT encoder on VQA 2.0, which might be due to the fact that VQA 2.0 is less challenging and therefore did not cause the BERT base network to drift as much compared to when we train the model on GQA.

% \citet{hudson2019gqa} suggest that different input representations have an impact on the results. They indicate that the more high-level and semantic representations lead to better results. This might be the reason why MCAN with BERT features performs better than with BERT encoder.

In comparison to the overall accuracy scores on VQA 2.0, we can see a significant drop on GQA. This is because the questions in GQA tend to be more challenging, as they are more compositional compared to VQA 2.0 and involve a wide array of reasoning skills (i.e.\, object and attribute recognition, transitive relation tracking, spatial reasoning, logical inference and comparisons). \citet{hudson2019gqa} also argued that open questions are not balanced in the VQA 2.0 dataset. To address this issue, the open questions in GQA are balanced by applying a smoothing technique in order to make the answer distribution for each question group more uniform. As a result, GQA is more robust against shortcuts and guesses, which are often exploited by VQA models when facing difficult learning problems \citep{agrawal12018gvqa}. As seen from Table \ref{table:gqa_result}, both ViLBERT and MCAN score relatively low for the open questions category. This suggests that there is still room for improvement in modeling fine-grained recognition and commonsense reasoning, which are both critical for answering open-ended questions.

%Seeing that the accuracy for the balanced open questions on the GQA dataset is relatively low, this might also contribute to the overall lower accuracy score on GQA.

% In VQA 2.0 images could be ignored for answering questions

%from GQA paper: longer (MAC) networks with more cells are better at GQA task. how do MCAN and ViLBERT compare in length? Does this also affect memory?
%from GQA paper: Also larger size of training set gives better performance for GQA

%reason why ViLBERT is better: maybe multi-tasking, remembers more data, because it's trained on 12 datasets, generalizes well

\begin{table}[ht]
\captionsetup{singlelinecheck = false, justification=justified}
\setlength\tabcolsep{0pt} % let LaTeX figure out amount of inter-column whitespace
\label{turns2}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} l *{4}{d{2.4}} }
\toprule
 \multicolumn{1}{l}{Model} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{Binary} & \multicolumn{1}{c}{Open} & \multicolumn{1}{c}{Distribution}\\
\midrule
\midrule
MCAN-small (baseline) & 53.41 & 70.29 & 38.56 & 1.40 \\
MCAN /w BERT-base features  & 54.79 & 72.96 & 39.37 & 1.70 \\
MCAN /w BERT-large features  & 54.70 & 72.18 & 39.87 & 1.74\\
MCAN /w BERT-base encoder  & 53.62 & 71.12 & 38.77 & 1.77\\
\midrule
Multi-task ViLBERT  & 59.75\ast & 78.49\ast & 43.85\ast & 1.31\ast \\
\bottomrule
\end{tabular*}
\caption{Results for MCAN models and multi-task ViLBERT on the GQA dataset \citep{hudson2019gqa}. All results refer to the balanced \textit{testdev} set. Models are evaluated for overall accuracy as well as accuracies for two different answer types (binary, open). In addition, they are evaluated by the distribution metric which is calculated using Chi-Square statistic (lower is better). Asterisk denotes the best performance on each metric.} 
\label{table:gqa_result}
\end{table}

\subsection{Question Grounding} \label{subsection:grounding}
% bert-base encoder score is even lower than mcan-small. this confirm our suspicion of catastrophic forgetting
% can also relate to information streams?
% compare pooled reps in vilbert to attention reduction module?
%combining multi task training and attention reduction module to improve grounding?

Table \ref{table:gqa_grounding} summarizes our results for question grounding on the GQA dataset. The grounding score indicates whether the model actively attends to regions in the image that are relevant for answering the question. Hence, it shows to what extent the model grounds its reasoning in the image. The MCAN model with BERT-base features performs best on grounding and significantly outperforms multi-task ViLBERT by over 30 points. This is especially interesting, as multi-task ViLBERT has consistently outperformed all MCAN models on the overall accuracy scores on VQA 2.0 and GQA. Even if we consider the best attention head of ViLBERT, there is still a difference of over 10 points to our best-performing MCAN model.

We hypothesize that the attention reduction module of MCAN helps to summarize the information in all the attention heads and refocus the model to attend to the relevant image regions. This can be examined from the grounding score of MCAN with BERT as encoder, which performs the worst but still managed to score 4.17 points higher compared to the best attention head of ViLBERT. This also indicates that joint representations learned by a task-specific model are better at conceptual grounding. Furthermore, this demonstrates that a task-specific model relies less on memorizing statistical biases in the dataset and grounds most of its reasoning in the image.

%One advantage of MCAN might be its attention reduction model which helps to summarize all the attention heads. The scores of both MCAN models with BERT features are quite similar compared to the remaining MCAN models. This could indicate that feature representations for task-specific models improve grounding. They might be better at not remembering previous questions and be more precise at relating textual and visual information.  

We also observe the grounding scores for all 8 attention heads of multi-task ViLBERT in order to obtain a better insight on the multi-headed attention mechanism. We find that out of all heads, only 2 heads are actively paying attention to the relevant visual regions (head 1 and 7). This is consistent with previous findings from \citet{li2020bert}, where they demonstrate that only certain attention heads of ViLBERT-like architectures actively ground elements of language to image regions, as the attention heads specialize in different things. They also discover that attention heads in upper layers tend to have higher grounding accuracy. 

%\citet{clark2019bert} discovered that some attention heads of BERT in relation to text, especially in lower layers, have very broad attention. If we translate this behavior to multi-task ViLBERT's first attention head, then this would mean that ViLBERT gets a higher grounding score on broad attention. % not sure if the last part makes sense

Multi-task ViLBERT might be good at generalizing due to its bidirectional information flows and training objective which has regularizing effects, but it lacks the ability to focus its attention on relevant regions of the image when answering the question. It is remarkable that multi-task ViLBERT still manages to score a high overall accuracy on GQA and VQA 2.0 compared to MCAN, despite not truly grounding its reasoning in the images. Multi-task ViLBERT's over-reliance on language might also be influenced by its weight initialization, which is obtained by pre-training with masked multi-modal modeling and multi-modal alignment prediction. During pre-training, visual regions with significant overlap are aggressively masked to avoid leaking visual information. It seems that this combined with the already available language representation from the ViLBERT linguistic stream forces the model to rely more heavily on language. %Another reason for multi-task ViLBERT's high accuracy score might be that it remembers previous questions and is therefore more likely to give a correct answer.

% ViLBERT evaluates visual grounding consistency by measuring the overlap in visual concepts between a question and reference (referring expression of image), they count overlapping nouns and adjectives. Why is this only based on language (without looking at the image)?  

% larger word representations can help the grounding, 
%catastrophic forgetting?

\begin{table}[ht]
\captionsetup{singlelinecheck = false, justification=justified}
\setlength\tabcolsep{0pt} % let LaTeX figure out amount of inter-column whitespace
\label{turns3}
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} l *{3}{d{2.4}} }
\toprule
 \multicolumn{1}{l}{Model} & \multicolumn{1}{c}{Attention Head} & \multicolumn{1}{c}{Grounding}\\
\midrule
\midrule
MCAN-small (baseline)  & & 86.34 \\
MCAN /w BERT-base features  & & 90.90\ast \\
MCAN /w BERT-large features  & & 89.61 \\
MCAN /w BERT-base encoder  & & 83.90 \\
\midrule
Multi-task ViLBERT  & 1 & 73.35 \\
& 2 & 54.33 \\
& 3 & 48.46 \\
& 4 & 49.43 \\
& 5 & 47.52 \\
& 6 & 54.12 \\
& 7 & 79.73 \\
& 8 & 57.75 \\
\midrule
Multi-task ViLBERT (averaged) & & 58.09 \\
\bottomrule
\end{tabular*}
\caption{Results for MCAN models and multi-task ViLBERT on grounding based on the GQA dataset \citep{hudson2019gqa}. The grounding score shows how successful the model attends to regions within the image that are relevant to the question. We evaluate the grounding score on the \textit{val} set, as there is no scene graph annotation available for the \textit{testdev} set. In addition to the overall grounding scores, we also show the grounding scores for 8 attention heads of multi-task VilBERT. Asterisk denotes the best performance.}
\label{table:gqa_grounding}
\end{table}

\subsection{Ablations of Fusion Method}

We present ablations of fusion method for both MCAN and ViLBERT in Table \ref{table:ablation_vqa} and \ref{table:ablation_gqa}. For ease of comparison, we also include the result with unmodified fusion method (element-wise product for multi-task ViLBERT and element-wise sum for MCAN). As expected, for multi-task ViLBERT, element-wise sum does not perform well as it is trained with element-wise product strategy for both VQA 2.0 and GQA. Surprisingly, the performance of ViLBERT on the yes/no question category of VQA 2.0 only degrades by 1.49 points. This shows that questions with binary answers in VQA 2.0 are `easier' to solve compared to other question categories. The `other' question category in VQA 2.0 stands out as being specifically hard to solve for multi-task ViLBERT with element-wise sum with a very low score of only 5.94 points.

We lack sufficient evidence to suggest whether element-wise sum or element-wise product is better for cross-modal feature fusion, as the result for MCAN is contradictory for VQA 2.0 and GQA. We observe that while for VQA using element-wise sum results in better performance, for GQA it is the opposite. The only exception occurs when we apply BERT-large as features with element-wise product for GQA, which might be due to the low dimensionality of the LSTM encoder. 

\begin{table}[ht]
\captionsetup{singlelinecheck = false, justification=justified}
\setlength\tabcolsep{0pt} % let LaTeX figure out amount of inter-column whitespace
\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} l *{4}{d{2.4}} }
\toprule
 \multicolumn{1}{l}{Model} & \multicolumn{1}{c}{Overall} & \multicolumn{1}{c}{Y/N} & \multicolumn{1}{c}{Num} & \multicolumn{1}{c}{Other}\\
\midrule
\midrule
\textbf{MCAN /w BERT-base features} $+$ & 70.70 & 86.82 & 52.94 & 60.96 \\
MCAN /w BERT-base features $\odot$ & 70.28 & 86.35 & 52.71 & 60.53 \\
\textbf{MCAN /w BERT-large features} $+$ & 70.53 & 86.51 & 53.01 & 60.86  \\
MCAN /w BERT-large features $\odot$ & 70.11 & 86.25 & 52.90 & 60.22\\
\textbf{MCAN /w BERT-base encoder} $+$ & 71.03 & 87.34 & 53.30 & 61.12 \\
MCAN /w BERT-base encoder $\odot$ & 70.51 & 87.00 & 53.15 & 60.35 \\
\midrule
\textbf{Multi-task ViLBERT} $\odot$ & 72.57 & 88.33 & 56.72 & 62.68 \\
Multi-task ViLBERT $+$ & 43.72 & 86.84 & 45.20 & 5.94 \\
\bottomrule
\end{tabular*}
\caption{Comparison of performance between MCAN and ViLBERT with element-wise sum and element-wise product fusion on VQA 2.0. The unmodified methods are denoted with \textbf{bold}. $+$ denotes element-wise sum while $\odot$ denotes element-wise product.} %make original italic
\label{table:ablation_vqa}

\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}} l *{4}{d{2.4}} }
\toprule
 \multicolumn{1}{l}{Model} & \multicolumn{1}{c}{Accuracy} & \multicolumn{1}{c}{Binary} & \multicolumn{1}{c}{Open} & \multicolumn{1}{c}{Distribution}\\
\midrule
\midrule
\textbf{MCAN /w BERT-base features} $+$ & 54.79 & 72.96 & 39.37 & 1.70 \\
MCAN /w BERT-base features $\odot$ & 55.76 & 73.76 & 40.50 & 1.55 \\
\textbf{MCAN /w BERT-large features} $+$ & 54.70 & 72.18 & 39.87 & 1.74  \\
MCAN /w BERT-large features $\odot$ & 54.33 & 72.18 & 39.19 & 1.76\\
\textbf{MCAN /w BERT-base encoder} $+$ & 53.62 & 71.12 & 38.77 & 1.77 \\
MCAN /w BERT-base encoder $\odot$ & 54.14 & 71.80 & 39.16 & 1.58 \\
\midrule
\textbf{Multi-task ViLBERT} $\odot$ & 59.75 & 78.49 & 43.85 & 1.31 \\
Multi-task ViLBERT $+$ &  42.58 & 67.63 & 21.34 & 8.32\\
\bottomrule
\end{tabular*}
\caption{Comparison of performance between MCAN and ViLBERT with element-wise sum and element-wise product fusion on GQA. The unmodified methods are denoted with \textbf{bold}. $+$ denotes element-wise sum while $\odot$ denotes element-wise product.} %make original italic
\label{table:ablation_gqa}
\end{table}

\subsection{Qualitative Analysis}
We provide a qualitative comparison of visual grounding between MCAN and multi-task ViLBERT on the GQA \textit{testdev} set. In order to do this, we use the learned attention from our best-performing MCAN model and multi-task ViLBERT. We did not include comparison for VQA 2.0 as it is highly unreliable due to strong priors which are exploited by the models for inference, instead of relying on visual understanding \citep{yinyang, agrawal12018gvqa}. 

For MCAN, we visualize the learned attentions in the image attention reduction module (Eq. (\ref{eq_mcan_att})). We highlight the image region where the model focuses its attention the most and blur out the area where the attention score is low. As ViLBERT does not have the attention reduction module, we visualize the learned attention in a different manner. We use attention head 7 from the last layer, as it performs the best in \S \ref{subsection:grounding} and visualize the sentence to image co-attention. For each concept/object that exists in the image, we take the 10 most attended regions for that concept/object and show the corresponding image patches. Figure \ref{fig:att_visu} shows several examples of learned attentions for MCAN and multi-task ViLBERT.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.8]{att_visu}
	\caption{Examples of attention visualization for MCAN and ViLBERT on GQA \textit{testdev} set. From left to right are the original image-question pair, prediction and the learned image attention for multi-task ViLBERT, prediction and the learned image attention for MCAN. For ViLBERT, the bounding boxes correspond to the sentence to image attention of the highlighted words in the question. We use different bounding box color when the attention is shared between different words. As for MCAN, we visualize the attention learned by Eq. (\ref{eq_mcan_att}) and highlight the region with highest attention score.}
	\label{fig:att_visu}
\end{figure}

From the examples, we can see the difference of grounding capability between MCAN and multi-task ViLBERT. Although ViLBERT performs better compared to MCAN in GQA, it does not actively focus on the concept/object relevant for answering the question most of the time (e.g.,\ in the first question-image pair the attention maps for `clothing' do not focus on the correct object in the image and in the third example, the attention maps for `small side table' do not focus on any of the tables). Occasionally, ViLBERT demonstrates the capability to learn meaningful visual grounding. This happens in the last example, where ViLBERT is able to show the relation between `food' and `cake' and their corresponding image regions, although it outputs the incorrect prediction. We hypothesize that for `easy' questions, ViLBERT does not actively ground its reasoning in images as much as it should while for the harder ones, ViLBERT focuses on the region that is relevant for answering the question. This is possibly caused by ViLBERT utilizing other attention heads for `easier' questions, and using this head for questions that require fine-grained recognition.

On the other hand, MCAN consistently grounds its reasoning in the image. For both correct and incorrect predicted examples, the attention maps focus on the most relevant image regions for answering the question. For instance, in the third example MCAN mostly attends to the left side of the room, with a particular focus on the table. This is also followed by predicting `left' for the answer, which is consistent with the attention maps albeit incorrect.

\section{Conclusion and Future Work}

In this work, we investigate the properties of joint multimodal representations derived from both a task-specific model and a multi-task model with respect to different training objective and information streams. We compare MCAN and multi-task ViLBERT on the VQA task and evaluate their performance on the VQA 2.0 and GQA datasets. The results give us an insight into the diverse improvements of the joint multimodal representations induced by the different architectures.

We show that on the one hand, ViLBERT improves the representations due to its bidirectional information streams and multi-task training, which also has a regularizing effect on the model and helps with learning underlying associations between language and visual concepts (e.g.,\ for relations that rarely appear in VQA tasks). Although this results in high accuracy, we also demonstrate that multi-task ViLBERT does not actively ground its reasoning in images through the GQA grounding metric. On the other hand, we postulate that MCAN improves the representations by summarizing the information in various attention heads with the attention reduction module, which helps the model to refocus on the most important information and thus improves conceptual grounding. Furthermore, we observed only a modest increase in performance when applying BERT to MCAN, which indicates that the substantial improvement that BERT offers to various NLP tasks might not necessarily translate to language and vision tasks. %In addition, we noticed that the learning rate used for BERT encoder might damage the learned language representations in BERT due to catastrophic forgetting. 

For future work, it would be interesting to further exploit the bidirectional characteristic of BERT embeddings by passing them through a bidirectional LSTM to obtain better question representations. In addition, it might also be worth to explore other contextual word representations derived from better language models such as RoBERTa \citep{liu2019roberta}. Evaluating the models on more VQA datasets such as TDIUC \citep{kafle2017tdiuc} and VQA-CP \citep{agrawal12018gvqa}, which we briefly discussed in \S \ref{subsection:vqa}, could also give us a deeper insight into the properties of representations specifically in regards to image reasoning and understanding. The results from VQA-CP would be especially useful to ascertain whether the underlying associations between language and visual concepts learned by multi-task ViLBERT are truly grounded, as the answer distributions are extremely dissimilar between training and validation sets. Lastly, applying Grad-CAM (Gradient-weighted Class Activation Mapping) \citep{selvaraju2017gradcam} for the visualization of attention maps to our models would allow us to compare their attention maps to human attention maps via rank correlation \citep{das2016human}.

%future work:
% probing task

%In this project/report, we investigated whether joint representations derived from pre-trained models are better than joint representations learned from a task-specific model, by comparing multi-task ViLBERT and MCAN on the VQA task. We define better by being more successful in grounding their answers based on reasoning on images, thus not (solely on) getting a higher accuracy score (/being better at giving the correct answer). Since VQA systems are prone to ignore images while still giving a correct answer (due to superficial correlation), the overall accuracy score is not sufficient to answer our research question. Evaluating both models on grounding allowed us to gain more insight into their ability to successfully base their reasoning on (the) images. 
\bibliography{sempix}

\end{document}
