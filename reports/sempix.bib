@inProceedings{yu2019mcan,
  author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
  title = {Deep Modular Co-Attention Networks for Visual Question Answering},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {6281--6290},
  year = {2019}
}

@inproceedings{lu2019vilbert,
  title={Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks},
  author={Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13--23},
  year={2019}
}

@InProceedings{lu2020multitask,
author = {Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
title = {12-in-1: Multi-Task Vision and Language Representation Learning},
booktitle = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{antol2015vqa,
author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C. and Parikh, Devi},
title = {VQA: Visual Question Answering},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {December},
year = {2015}
} 

@incollection{malinowski2014vqa,
title = {A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input},
author = {Malinowski, Mateusz and Fritz, Mario},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {1682--1690},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5411-a-multi-world-approach-to-question-answering-about-real-world-scenes-based-on-uncertain-input.pdf}
}

@inproceedings{zhao2018vqa,
  title     = {Open-Ended Long-form Video Question Answering via Adaptive Hierarchical Reinforced Networks},
  author    = {Zhou Zhao and Zhu Zhang and Shuwen Xiao and Zhou Yu and Jun Yu and Deng Cai and Fei Wu and Yueting Zhuang},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},             
  pages     = {3683--3689},
  year      = {2018},
  month     = {7},
  doi       = {10.24963/ijcai.2018/512},
  url       = {https://doi.org/10.24963/ijcai.2018/512},
}

@article{zellers2019vcr,
  title={From Recognition to Cognition: Visual Commonsense Reasoning},
  author={Rowan Zellers and Yonatan Bisk and Ali Farhadi and Yejin Choi},
  journal={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019},
  pages={6713-6724}
}

@inproceedings{wang2016retrieval,
  author={L. {Wang} and Y. {Li} and S. {Lazebnik}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, title={Learning Deep Structure-Preserving Image-Text Embeddings}, 
  year={2016},
  volume={},
  number={},
  pages={5005-5013},
}

@article{xie2019entailment,
  author    = {Ning Xie and
               Farley Lai and
               Derek Doran and
               Asim Kadav},
  title     = {Visual Entailment: {A} Novel Task for Fine-Grained Image Understanding},
  journal   = {CoRR},
  volume    = {abs/1901.06706},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.06706},
  archivePrefix = {arXiv},
  eprint    = {1901.06706},
  timestamp = {Fri, 01 Feb 2019 13:39:59 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1901-06706.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{agrawal12018gvqa,
author = {Agrawal, Aishwarya and Batra, Dhruv and Parikh, Devi and Kembhavi, Aniruddha},
title = {Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@inproceedings{yang2016vqa,
	author={Z. {Yang} and X. {He} and J. {Gao} and L. {Deng} and A. {Smola}},	
	booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 	
	title={Stacked Attention Networks for Image Question Answering}, 	
	year={2016},	
	volume={},	
	number={},	
	pages={21-29},
}

@article{hudson2018mac,
  title={Compositional Attention Networks for Machine Reasoning},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{hudson2019gqa,
  title={GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering},
  author={Hudson, Drew A and Manning, Christopher D},
  journal={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}

@article{goyal2017vqa2,
  title={Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering},
  author={Yash Goyal and Tejas Khot and Douglas Summers-Stay and Dhruv Batra and D. Parikh},
  journal={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017},
  pages={6325-6334}
}

@article{yu2018beyond,
  title={Beyond Bilinear: Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering},
  author={Yu, Zhou and Yu, Jun and Xiang, Chenchao and Fan, Jianping and Tao, Dacheng},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={29},
  number={12},
  pages={5947--5959},
  year={2018}
}

@InProceedings{Nguyen_2018_CVPR,
author = {Nguyen, Duy-Kien and Okatani, Takayuki},
title = {Improved Fusion of Visual and Language Representations by Dense Symmetric Co-Attention for Visual Question Answering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@incollection{ban,
title = {Bilinear Attention Networks},
author = {Kim, Jin-Hwa and Jun, Jaehyun and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {1564--1574},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7429-bilinear-attention-networks.pdf}
}

@incollection{transformers,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@article{dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@INPROCEEDINGS{residual,  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Deep Residual Learning for Image Recognition},   year={2016},  volume={},  number={},  pages={770-778},}

@misc{ba2016layer,
    title={Layer Normalization},
    author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year={2016},
    eprint={1607.06450},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@InProceedings{Anderson_2018_CVPR,
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@incollection{faster_rcnn,
title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {91--99},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{mogadala2019vqa-survey,
	author    = {Aditya Mogadala and
	Marimuthu Kalimuthu and
	Dietrich Klakow},
	title     = {Trends in Integration of Vision and Language Research: {A} Survey
	of Tasks, Datasets, and Methods},
	journal   = {CoRR},
	volume    = {abs/1907.09358},
	year      = {2019},
	url       = {http://arxiv.org/abs/1907.09358},
	archivePrefix = {arXiv},
	eprint    = {1907.09358},
	timestamp = {Tue, 30 Jul 2019 12:52:26 +0200},
	biburl    = {https://dblp.org/rec/journals/corr/abs-1907-09358.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{mikolov2013efficient,
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	biburl = {https://www.bibsonomy.org/bibtex/28b132b4b7e82cfb538fd462887ba98b8/florianpircher},
	description = {Efficient Estimation of Word Representations in Vector Space},
	interhash = {e92df552b17e9f952226a893b84ad739},
	intrahash = {8b132b4b7e82cfb538fd462887ba98b8},
	keywords = {final thema:sequence_labeling word_embedding},
	note = {cite arxiv:1301.3781},
	timestamp = {2018-11-27T09:35:06.000+0100},
	title = {Efficient Estimation of Word Representations in Vector Space},
	url = {http://arxiv.org/abs/1301.3781},
	year = {2013}
}

@inproceedings{plummer2015vdg,
	author = {Plummer, Bryan A. and Wang, Liwei and Cervantes, Chris M. and Caicedo, Juan C. and Hockenmaier, Julia and Lazebnik, Svetlana},
	title = {Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models},
	year = {2015},
	isbn = {9781467383912},
	publisher = {IEEE Computer Society},
	address = {USA},
	url = {https://doi.org/10.1109/ICCV.2015.303},
	doi = {10.1109/ICCV.2015.303},
	booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
	pages = {2641–2649},
	numpages = {9},
	series = {ICCV '15}
}

@inproceedings{fitzgerald2013learning,
	title = {Learning Distributions over Logical Forms for Referring Expression Generation},
	author = {FitzGerald, Nicholas  and
	Artzi, Yoav  and
	Zettlemoyer, Luke},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	year = {2013},
	address = {Seattle, Washington, USA},
	publisher = {Association for Computational Linguistics},
	url = {https://www.aclweb.org/anthology/D13-1197},
	pages = {1914--1925},
}

@inproceedings{nagaraja2016modeling,
author={Nagaraja, Varun K.
and Morariu, Vlad I.
and Davis, Larry S.},
editor={Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
title={Modeling Context Between Objects for Referring Expression Understanding},
booktitle={Computer Vision -- ECCV 2016},
year={2016},
publisher={Springer International Publishing},
address={Cham},
pages={792--807},
abstract={Referring expressions usually describe an object using properties of the object and relationships of the object with other objects. We propose a technique that integrates context between objects to understand referring expressions. Our approach uses an LSTM to learn the probability of a referring expression, with input features from a region and a context region. The context regions are discovered using multiple-instance learning (MIL) since annotations for context objects are generally not available for training. We utilize max-margin based MIL objective functions for training the LSTM. Experiments on the Google RefExp and UNC RefExp datasets show that modeling context between objects provides better performance than modeling only object properties. We also qualitatively show that our technique can ground a referring expression to its referred region along with the supporting context region.},
isbn={978-3-319-46493-0}
}

@inproceedings{yu2016modeling,
author={Yu, Licheng
and Poirson, Patrick
and Yang, Shan
and Berg, Alexander C.
and Berg, Tamara L.},
editor={Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
title={Modeling Context in Referring Expressions},
booktitle={Computer Vision -- ECCV 2016},
year={2016},
publisher={Springer International Publishing},
address={Cham},
pages={69--85},
abstract={Humans refer to objects in their environments all the time, especially in dialogue with other people. We explore generating and comprehending natural language referring expressions for objects in images. In particular, we focus on incorporating better measures of visual context into referring expression models and find that visual comparison to other objects within an image helps improve performance significantly. We also develop methods to tie the language generation process together, so that we generate expressions for all objects of a particular category jointly. Evaluation on three recent datasets - RefCOCO, RefCOCO+, and RefCOCOg (Datasets and toolbox can be downloaded from https://github.com/lichengunc/refer), shows the advantages of our methods for both referring expression generation and comprehension.},
isbn={978-3-319-46475-6}
}

@inproceedings{johnson2017clevr,
  title={CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
  author={Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens
          and Fei-Fei, Li and Zitnick, C Lawrence and Girshick, Ross},
  booktitle={CVPR},
  year={2017}
}

@article {geman2015visual,
	author = {Geman, Donald and Geman, Stuart and Hallonquist, Neil and Younes, Laurent},
	title = {Visual Turing test for computer vision systems},
	volume = {112},
	number = {12},
	pages = {3618--3623},
	year = {2015},
	doi = {10.1073/pnas.1422953112},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/112/12/3618},
	eprint = {https://www.pnas.org/content/112/12/3618.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{manmadhan2020vqa,
	author = {Manmadhan, Sruthy and Kovoor, Binsu},
	year = {2020},
	month = {04},
	pages = {},
	title = {Visual question answering: a state-of-the-art review},
	journal = {Artificial Intelligence Review},
	doi = {10.1007/s10462-020-09832-7}
}

@incollection{krizhevsky2012imagenet,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	author = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {1097--1105},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@article{russakovsky2015imagenet,
	Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	Title = {{ImageNet Large Scale Visual Recognition Challenge}},
	Year = {2015},
	journal   = {International Journal of Computer Vision (IJCV)},
	doi = {10.1007/s11263-015-0816-y},
	volume={115},
	number={3},
	pages={211-252}
}

@inproceedings{zeiler2014visual,
author={Zeiler, Matthew D.
and Fergus, Rob},
editor={Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne},
title={Visualizing and Understanding Convolutional Networks},
booktitle={Computer Vision -- ECCV 2014},
year={2014},
publisher={Springer International Publishing},
address={Cham},
pages={818--833},
abstract={Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
isbn={978-3-319-10590-1}
}

@article{simonyan2015very,
	title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
	author={Simonyan, Karen and Zisserman, Andrew},
	journal={international conference on learning representations},
	year={2015}
}

@article{szegedy2015going,
  title={Going deeper with convolutions},
  author={Christian Szegedy and W. Liu and Y. Jia and Pierre Sermanet and Scott Reed and Dragomir Anguelov and D. Erhan and V. Vanhoucke and Andrew Rabinovich},
  journal={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015},
  pages={1-9}
}

@article{miller1991contextual,
	title={Contextual correlates of semantic similarity},
	author={G. Miller and W. Charles},
	journal={Language and Cognitive Processes},
	year={1991}
}

@inproceedings{peters2018elmo,
    title = {Deep Contextualized Word Representations},
    author = {Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    month = jun,
    year = {2018},
    address = {New Orleans, Louisiana},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N18-1202},
    doi = {10.18653/v1/N18-1202},
    pages = {2227--2237},
    abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
}

@inproceedings{cho2014gru,
    title = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
    month = oct,
    year = {2014},
    address = {Doha, Qatar},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D14-1179},
    doi = {10.3115/v1/D14-1179},
    pages = {1724--1734},
}

@article{elman1990finding,
title = {Finding structure in time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179 - 211},
year = {1990},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(90)90002-E},
url = {http://www.sciencedirect.com/science/article/pii/036402139090002E},
author = {Jeffrey L. Elman},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}

@article{young2018recent,
  author={T. {Young} and D. {Hazarika} and S. {Poria} and E. {Cambria}},
  journal={IEEE Computational Intelligence Magazine}, 
  title={Recent Trends in Deep Learning Based Natural Language Processing [Review Article]}, 
  year={2018},
  volume={13},
  number={3},
  pages={55-75},
}

@inproceedings{sharma-etal-2018-conceptual,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}


@article{krishnavisualgenome,
author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and Bernstein, Michael and Fei-Fei, Li},
year = {2017},
title = {Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations},
journal = {International Journal of Computer Vision},
volume = {123},
pages = {32--73},
url = {https://doi.org/10.1007/s11263-016-0981-7},
}

@InProceedings{coco,
author="Lin, Tsung-Yi
and Maire, Michael
and Belongie, Serge
and Hays, James
and Perona, Pietro
and Ramanan, Deva
and Doll{\'a}r, Piotr
and Zitnick, C. Lawrence",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Microsoft COCO: Common Objects in Context",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="740--755",
abstract="We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.",
isbn="978-3-319-10602-1"
}

@inproceedings{kazemzadeh-etal-2014-referitgame,
    title = "{R}efer{I}t{G}ame: Referring to Objects in Photographs of Natural Scenes",
    author = "Kazemzadeh, Sahar  and
      Ordonez, Vicente  and
      Matten, Mark  and
      Berg, Tamara",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1086",
    doi = "10.3115/v1/D14-1086",
    pages = "787--798",
}

@INPROCEEDINGS{maorefcoco,  author={J. {Mao} and J. {Huang} and A. {Toshev} and O. {Camburu} and A. {Yuille} and K. {Murphy}},  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},   title={Generation and Comprehension of Unambiguous Object Descriptions},   year={2016},  volume={},  number={},  pages={11-20},}

@InProceedings{zhu2016cvpr,
  title = {{Visual7W: Grounded Question Answering in Images}},
  author = {Yuke Zhu and Oliver Groth and Michael Bernstein and Li Fei-Fei},
  booktitle = {{IEEE Conference on Computer Vision and Pattern Recognition}},
  year = 2016,
}

@inproceedings{guesswhat_game,
author = {Harm de Vries and Florian Strub and Sarath Chandar and Olivier Pietquin and Hugo Larochelle and Aaron C. Courville},
title = {GuessWhat?! Visual object discovery through multi-modal dialogue},
booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
year = {2017}
}

@inproceedings{suhr-etal-2019-corpus,
    title = "A Corpus for Reasoning about Natural Language Grounded in Photographs",
    author = "Suhr, Alane  and
      Zhou, Stephanie  and
      Zhang, Ally  and
      Zhang, Iris  and
      Bai, Huajun  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1644",
    doi = "10.18653/v1/P19-1644",
    pages = "6418--6428",
    abstract = "We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",
}

@misc{xie2018visual,
    title={Visual Entailment Task for Visually-Grounded Language Learning},
    author={Ning Xie and Farley Lai and Derek Doran and Asim Kadav},
    year={2018},
    eprint={1811.10582},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@article{bengio2013represent,
  author={Y. {Bengio} and A. {Courville} and P. {Vincent}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Representation Learning: A Review and New Perspectives}, 
  year={2013},
  volume={35},
  number={8},
  pages={1798-1828}
}

@inproceedings{mikolov2013distri,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed Representations of Words and Phrases and Their Compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111–3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@article{dmello2015review,
author = {D'mello, Sidney K. and Kory, Jacqueline},
title = {A Review and Meta-Analysis of Multimodal Affect Detection Systems},
year = {2015},
issue_date = {April 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2682899},
doi = {10.1145/2682899},
journal = {ACM Comput. Surv.},
month = feb,
articleno = {43},
numpages = {36},
keywords = {survey, evaluation, Affective computing, human-centered computing, methodology}
}

@inproceedings{silberer2014learning,
    title = {Learning Grounded Meaning Representations with Autoencoders},
    author = {Silberer, Carina  and
      Lapata, Mirella},
    booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {jun},
    year = {2014},
    address = {Baltimore, Maryland},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/P14-1068},
    doi = {10.3115/v1/P14-1068},
    pages = {721--732},
}

@inproceedings{mroueh2015deep,
  author={Y. {Mroueh} and E. {Marcheret} and V. {Goel}},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Deep multimodal learning for Audio-Visual Speech Recognition}, 
  year={2015},
  volume={},
  number={},
  pages={2130-2134}
}
  
@inproceedings{ngiam2011multimodal,
  title={Multimodal Deep Learning},
  author={J. Ngiam and A. Khosla and Mingyu Kim and Juhan Nam and H. Lee and A. Ng},
  booktitle={ICML},
  year={2011}
}

@inproceedings{wu2014exploring,
  title={Exploring Inter-feature and Inter-class Relationships with Deep Neural Networks for Video Classification},
  author={Zuxuan Wu and Yu-Gang Jiang and Jun Wang and Jian Pu and X. Xue},
  booktitle={MM '14},
  year={2014}
}

@InProceedings{salakhutdinov2009boltz,
  title = 	 {Deep Boltzmann Machines},
  author = 	 {Ruslan Salakhutdinov and Geoffrey Hinton},
  pages = 	 {448--455},
  year = 	 {2009},
  editor = 	 {David van Dyk and Max Welling},
  volume = 	 {5},
  booktitle = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/salakhutdinov09a/salakhutdinov09a.pdf},
  url = 	 {http://proceedings.mlr.press/v5/salakhutdinov09a.html},
  abstract = 	 {We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and data-independent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized by a single bottom-up pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.}
}

@inproceedings{chen2015multi,
author = {Chen, Shizhe and Jin, Qin},
title = {Multi-Modal Dimensional Emotion Recognition Using Recurrent Neural Networks},
year = {2015},
isbn = {9781450337434},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2808196.2811638},
doi = {10.1145/2808196.2811638},
booktitle = {Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge},
pages = {49--56},
numpages = {8},
keywords = {affective computing, emotion recognition, recurrent neural network},
location = {Brisbane, Australia},
series = {AVEC '15}
}

@article{nicolaou2011contin,
  author={M. A. {Nicolaou} and H. {Gunes} and M. {Pantic}},
  journal={IEEE Transactions on Affective Computing}, 
  title={Continuous Prediction of Spontaneous Affect from Multiple Cues and Modalities in Valence-Arousal Space}, 
  year={2011},
  volume={2},
  number={2},
  pages={92-105}
 }
  
@InProceedings{rajagopalan2016extend,
author={Rajagopalan, Shyam Sundar
and Morency, Louis-Philippe
and Baltru\u{s}aitis, Tadas
and Goecke, Roland},
editor={Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
title={Extending Long Short-Term Memory for Multi-View Structured Learning},
booktitle={Computer Vision -- ECCV 2016},
year={2016},
publisher={Springer International Publishing},
address={Cham},
pages={338--353},
}

@inproceedings{weston2011wsabie,
author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
title = {WSABIE: Scaling up to Large Vocabulary Image Annotation},
year = {2011},
isbn = {9781577355151},
publisher = {AAAI Press},
abstract = {Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible annotations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at the top of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method, called WSABIE, both outperforms several baseline methods and is faster and consumes less memory.},
booktitle = {Proceedings of the Twenty-Second International Joint Conference on Artificial Intelligence - Volume Volume Three},
pages = {2764–2770},
numpages = {7},
location = {Barcelona, Catalonia, Spain},
series = {IJCAI'11}
}

@inproceedings{frome2013devise,
  title={DeViSE: A Deep Visual-Semantic Embedding Model},
  author={Andrea Frome and G. S. Corrado and Jonathon Shlens and S. Bengio and J. Dean and Marc'Aurelio Ranzato and Tomas Mikolov},
  booktitle={NIPS},
  year={2013}
}

@inproceedings{pan2016joint,
  author    = {Yingwei Pan and
               Tao Mei and
               Ting Yao and
               Houqiang Li and
               Yong Rui},
  title     = {Jointly Modeling Embedding and Translation to Bridge Video and Language},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {4594--4602},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.497},
  doi       = {10.1109/CVPR.2016.497},
  timestamp = {Mon, 18 Nov 2019 15:08:29 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/PanMYLR16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{wang2014hashing,
author = {Wang, Jingdong and Shen, Heng Tao and Song, Jingkuan and Ji, Jianqiu},
title = {Hashing for Similarity Search: A Survey},
year = {2014},
month = {August},
abstract = {Similarity search (nearest neighbor search) is a problem of pursuing the data items whose distances to a query item are the smallest from a large database. Various methods have been developed to address this problem, and recently a lot of efforts have been devoted to approximate search. In this paper, we present a survey on one of the main solutions, hashing, which has been widely studied since the pioneering work locality sensitive hashing. We divide the hashing algorithms two main categories: locality sensitive hashing, which designs hash functions without exploring the data distribution and learning to hash, which learns hash functions according the data distribution, and review them from various aspects, including hash function design and distance measure and search scheme in the hash coding space.},
url = {https://www.microsoft.com/en-us/research/publication/hashing-similarity-search-survey/},
}

@article{bronstein2010data,
  title={Data fusion through cross-modality metric learning using similarity-sensitive hashing},
  author={M. Bronstein and A. Bronstein and F. Michel and N. Paragios},
  journal={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  year={2010},
  pages={3594-3601}
}

@article{jiang2015class,
title = {The classification of multi-modal data with hidden conditional random field},
journal = {Pattern Recognition Letters},
volume = {51},
pages = {63 - 69},
year = {2015},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2014.08.005},
url = {http://www.sciencedirect.com/science/article/pii/S0167865514002517},
author = {Xinyang Jiang and Fei Wu and Yin Zhang and Siliang Tang and Weiming Lu and Yueting Zhuang},
keywords = {Hidden conditional random field, Latent structure, Multi-modal classification},
}

@article{hotelling1936relations,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2333955},
 author = {Harold Hotelling},
 journal = {Biometrika},
 number = {3/4},
 pages = {321--377},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Relations Between Two Sets of Variates},
 volume = {28},
 year = {1936}
}

@article{hardoon2004canonical,
  author={D. R. {Hardoon} and S. {Szedmak} and J. {Shawe-Taylor}},
  journal={Neural Computation}, 
  title={Canonical Correlation Analysis: An Overview with Application to Learning Methods}, 
  year={2004},
  volume={16},
  number={12},
  pages={2639-2664}
}

@InProceedings{klein2015associating,
author = {Klein, Benjamin and Lev, Guy and Sadeh, Gil and Wolf, Lior},
title = {Associating Neural Word Embeddings With Deep Image Representations Using Fisher Vectors},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Rasiwasia2010ANA,
  title={A new approach to cross-modal multimedia retrieval},
  author={Nikhil Rasiwasia and J. C. Pereira and E. Coviello and G. Doyle and G. Lanckriet and R. Levy and N. Vasconcelos},
  booktitle={ACM Multimedia},
  year={2010}
}

@article{sargin2007audio,
  author={M. E. {Sargin} and Y. {Yemez} and E. {Erzin} and A. M. {Tekalp}},
  journal={IEEE Transactions on Multimedia}, 
  title={Audiovisual Synchronization and Fusion Using Canonical Correlation Analysis}, 
  year={2007},
  volume={9},
  number={7},
  pages={1396-1403}
}

@incollection{slaney2001facesync,
title = {FaceSync: A Linear Operator for Measuring Synchronization of Video Facial Images and Audio Tracks},
author = {Malcolm Slaney and Michele Covell},
booktitle = {Advances in Neural Information Processing Systems 13},
editor = {T. K. Leen and T. G. Dietterich and V. Tresp},
pages = {814--820},
year = {2001},
publisher = {MIT Press},
url = {http://papers.nips.cc/paper/1921-facesync-a-linear-operator-for-measuring-synchronization-of-video-facial-images-and-audio-tracks.pdf}
}

@inproceedings{leong2011going,
    title ={Going Beyond Text: A Hybrid Image-Text Approach for Measuring Word Relatedness},
    author = {Leong, Chee Wee  and
      Mihalcea, Rada},
    booktitle = {Proceedings of 5th International Joint Conference on Natural Language Processing},
    month = {nov},
    year = {2011},
    address = {Chiang Mai, Thailand},
    publisher = {Asian Federation of Natural Language Processing},
    url = {https://www.aclweb.org/anthology/I11-1162},
    pages = {1403--1407},
}

@inproceedings{bruni2011distri,
author = {Bruni, Elia and Tran, Giang Binh and Baroni, Marco},
title = {Distributional Semantics from Text and Images},
year = {2011},
isbn = {9781937284169},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {We present a distributional semantic model combining text- and image-based features. We evaluate this multimodal semantic model on simulating similarity judgments, concept clustering and the BLESS benchmark. When integrated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, suggesting that the two sources of information are complementary.},
booktitle = {Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics},
pages = {22–32},
numpages = {11},
location = {Edinburgh, Scotland},
series = {GEMS '11}
}

@inproceedings{gunes2005affect,
  author={H. {Gunes} and M. {Piccardi}},
  booktitle={2005 IEEE International Conference on Systems, Man and Cybernetics}, 
  title={Affect recognition from face and body: early fusion vs. late fusion}, 
  year={2005},
  volume={4},
  number={},
  pages={3437-3443 Vol. 4},}

@inproceedings{snoek2005late,
author = {Snoek, Cees G. M. and Worring, Marcel and Smeulders, Arnold W. M.},
title = {Early versus Late Fusion in Semantic Video Analysis},
year = {2005},
isbn = {1595930442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1101149.1101236},
doi = {10.1145/1101149.1101236},
booktitle = {Proceedings of the 13th Annual ACM International Conference on Multimedia},
pages = {399–402},
numpages = {4},
keywords = {semantic concept detection, early fusion, multimedia understanding, late fusion},
location = {Hilton, Singapore},
series = {MULTIMEDIA '05}
}

@article{atrey2010hybrid,
	doi = {10.1007/s00530-010-0182-0},
	url = {https://doi.org/10.1007%2Fs00530-010-0182-0},
	year = 2010,
	month = {apr},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {16},
	number = {6},
	pages = {345--379},
	author = {Pradeep K. Atrey and M. Anwar Hossain and Abdulmotaleb El Saddik and Mohan S. Kankanhalli},
	title = {Multimodal fusion for multimedia analysis: a survey},
	journal = {Multimedia Systems}
}

@inproceedings{shutova2016black,
    title = {Black Holes and White Rabbits: Metaphor Identification with Visual Features},
    author = {Shutova, Ekaterina  and
      Kiela, Douwe  and
      Maillard, Jean},
    booktitle = {Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies},
    month = {jun},
    year = {2016},
    address = {San Diego, California},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N16-1020},
    doi = {10.18653/v1/N16-1020},
    pages = {160--170},
}

@InProceedings{morvant2014vote,
author={Morvant, Emilie
and Habrard, Amaury
and Ayache, St{\'e}phane},
editor={Fr{\"a}nti, Pasi
and Brown, Gavin
and Loog, Marco
and Escolano, Francisco
and Pelillo, Marcello},
title={Majority Vote of Diverse Classifiers for Late Fusion},
booktitle={Structural, Syntactic, and Statistical Pattern Recognition},
year={2014},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={153--162},
isbn={978-3-662-44415-3}
}

@ARTICLE{potamianos2003noise,
  author={G. {Potamianos} and C. {Neti} and G. {Gravier} and A. {Garg} and A. W. {Senior}},
  journal={Proceedings of the IEEE}, 
  title={Recent advances in the automatic recognition of audiovisual speech}, 
  year={2003},
  volume={91},
  number={9},
  pages={1306-1326}
}

@article{evangelopoulos2013variance,
	doi = {10.1109/tmm.2013.2267205},
	url = {https://doi.org/10.1109%2Ftmm.2013.2267205},
	year = 2013,
	month = {nov},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {15},
	number = {7},
	pages = {1553--1568},
	author = {Georgios Evangelopoulos and Athanasia Zlatintsi and Alexandros Potamianos and Petros Maragos and Konstantinos Rapantzikos and Georgios Skoumas and Yannis Avrithis},
	title = {Multimodal Saliency and Fusion for Movie Summarization Based on Aural, Visual, and Textual Attention},
	journal = {{IEEE} Transactions on Multimedia}
}

@InProceedings{glodek2011learned,
author={Glodek, Michael
and Tschechne, Stephan
and Layher, Georg
and Schels, Martin
and Brosch, Tobias
and Scherer, Stefan
and K{\"a}chele, Markus
and Schmidt, Miriam
and Neumann, Heiko
and Palm, G{\"u}nther
and Schwenker, Friedhelm},
editor={D'Mello, Sidney
and Graesser, Arthur
and Schuller, Bj{\"o}rn
and Martin, Jean-Claude},
title={Multiple Classifier Systems for the Classification of Audio-Visual Emotional States},
booktitle={Affective Computing and Intelligent Interaction},
year={2011},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={359--368},
isbn={978-3-642-24571-8}
}

@inproceedings{ramirez2011learned,
author = {Ramirez, Geovany A. and Baltru\v{s}aitis, Tadas and Morency, Louis-Philippe},
title = {Modeling Latent Discriminative Dynamic of Multi-Dimensional Affective Signals},
year = {2011},
isbn = {9783642245701},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
booktitle = {Proceedings of the 4th International Conference on Affective Computing and Intelligent Interaction - Volume Part II},
pages = {396--406},
numpages = {11},
keywords = {latent variable models, audio-visual emotion recognition, multi-modal fusion, conditional random fields},
location = {Memphis, TN},
series = {ACII'11}
}

@InProceedings{wu2005speaker,
author={Wu, Zhiyong
and Cai, Lianhong
and Meng, Helen},
editor={Zhang, David
and Jain, Anil K.},
title={Multi-level Fusion of Audio and Visual Features for Speaker Identification},
booktitle={Advances in Biometrics},
year={2005},
publisher={Springer Berlin Heidelberg},
address={Berlin, Heidelberg},
pages={493--499},
isbn={978-3-540-31621-3}
}

@article{lan2014multimedia,
author = {Lan, Zhen-zhong
and Bao, Lei
and Yu, Shoou-I
and Liu, Wei
and Hauptmann, Alexander},
year = {2014},
title = {Multimedia classification and event detection using double fusion},
journal = {Multimedia Tools and Applications},
volume = {71},
pages = {333--347},
url = {https://doi.org/10.1007/s11042-013-1391-2}
}

@article{goenen2011kernel,
author = {G\"{o}nen, Mehmet and Alpayd\i{}n, Ethem},
title = {Multiple Kernel Learning Algorithms},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = jul,
pages = {2211–2268},
numpages = {58}
}

@article{bucak2014kernel,
  author={S. S. {Bucak} and R. {Jin} and A. K. {Jain}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Multiple Kernel Learning for Visual Object Recognition: A Review}, 
  year={2014},
  volume={36},
  number={7},
  pages={1354-1369},}

@inproceedings{gehler2009kernel,
  author={P. {Gehler} and S. {Nowozin}},
  booktitle={2009 IEEE 12th International Conference on Computer Vision}, 
  title={On feature combination for multiclass object classification}, 
  year={2009},
  volume={},
  number={},
  pages={221-228},}

@inproceedings{chen2014recog,
author = {Chen, JunKai and Chen, Zenghai and Chi, Zheru and Fu, Hong},
title = {Emotion Recognition in the Wild with Feature Fusion and Multiple Kernel Learning},
year = {2014},
isbn = {9781450328852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2663204.2666277},
doi = {10.1145/2663204.2666277},
booktitle = {Proceedings of the 16th International Conference on Multimodal Interaction},
pages = {508--513},
numpages = {6},
keywords = {support vector machine, hog_top, multiple kernel learning, emotion recognition, feature fusion},
location = {Istanbul, Turkey},
series = {ICMI '14}
}

@inproceedings{jaques2015multi,
	author = {Jaques, N. and Taylor, S. and Sano, A. and Picard, R.W.},
	title = {Multi-task, Multi-Kernel Learning for Estimating Individual Wellbeing},
	booktitle = {NIPS Workshop on Multimodal Machine Learning},
	address = {Montreal, Quebec}, 
	year = {2015},
	url = {https://www.media.mit.edu/publications/multi-task-multi-kernel-learning-for-estimating-individual-wellbeing/}
}

Export Citation

@inproceedings{sikka2013multiple,
author = {Sikka, Karan and Dykstra, Karmen and Sathyanarayana, Suchitra and Littlewort, Gwen and Bartlett, Marian},
title = {Multiple Kernel Learning for Emotion Recognition in the Wild},
year = {2013},
isbn = {9781450321297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2522848.2531741},
doi = {10.1145/2522848.2531741},
booktitle = {Proceedings of the 15th ACM on International Conference on Multimodal Interaction},
pages = {517--524},
numpages = {8},
keywords = {support vector machine, multimodal, feature fusion, multiple kernel learning, bag of words},
location = {Sydney, Australia},
series = {ICMI '13}
}

@inproceedings{poria2015deep,
    title = {Deep Convolutional Neural Network Textual Features and Multiple Kernel Learning for Utterance-level Multimodal Sentiment Analysis},
    author = {Poria, Soujanya  and
      Cambria, Erik  and
      Gelbukh, Alexander},
    booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
    month = sep,
    year = {2015},
    address = {Lisbon, Portugal},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D15-1303},
    doi = {10.18653/v1/D15-1303},
    pages = {2539--2544},
}

@article{yeh2012novel,
author = {Yeh, Yi-Ren and Lin, Ting-Chu and Chung, Yung-Yu and Wang, Yu-Chiang Frank},
year = {2012},
month = {06},
pages = {563--574},
title = {A Novel Multiple Kernel Learning Framework for Heterogeneous Feature Fusion and Variable Selection},
volume = {14},
journal = {IEEE Transactions on Multimedia - TMM},
doi = {10.1109/TMM.2012.2188783}
}

@inproceedings{lafferty2001crf,
author = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
title = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
year = {2001},
isbn = {1558607781},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
pages = {282--289},
numpages = {8},
series = {ICML '01}
}

@article{quattoni2007hcrf,
author = {Quattoni, Ariadna and Wang, Sybor and Morency, Louis-Philippe and Collins, Michael and Darrell, Trevor},
title = {Hidden Conditional Random Fields},
year = {2007},
issue_date = {October 2007},
publisher = {IEEE Computer Society},
address = {USA},
volume = {29},
number = {10},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2007.1124},
doi = {10.1109/TPAMI.2007.1124},
abstract = {We present a discriminative latent variable model for classification problems in structured domains where inputs can be represented by a graph of local observations. A hidden-state Conditional Random Field framework learns a set of latent variables conditioned on local features. Observations need not be independent and may overlap in space and time.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = oct,
pages = {1848--1852},
numpages = {5},
keywords = {object recognition, classification, supervised learning, model}
}

@inproceedings{song2012multi,
author = {Song, Yale and Morency, Louis-Philippe and Davis, Randall},
title = {Multimodal Human Behavior Analysis: Learning Correlation and Interaction across Modalities},
year = {2012},
isbn = {9781450314671},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2388676.2388684},
doi = {10.1145/2388676.2388684},
booktitle = {Proceedings of the 14th ACM International Conference on Multimodal Interaction},
pages = {27--30},
numpages = {4},
keywords = {kernel methods, canonical correlation analysis, multimodal signal processing, multi-view latent variable discriminative models},
location = {Santa Monica, California, USA},
series = {ICMI '12}
}

@incollection{qin2009global,
title = {Global Ranking Using Continuous Conditional Random Fields},
author = {Qin, Tao and Tie-yan Liu and Xu-dong Zhang and De-sheng Wang and Li, Hang},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1281--1288},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3402-global-ranking-using-continuous-conditional-random-fields.pdf}
}
  
@inproceedings{gurban2008dynamic,
author = {Gurban, Mihai and Thiran, Jean-Philippe and Drugman, Thomas and Dutoit, Thierry},
title = {Dynamic Modality Weighting for Multi-Stream HMMs in Audio-Visual Speech Recognition},
year = {2008},
isbn = {9781605581989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1452392.1452442},
doi = {10.1145/1452392.1452442},
booktitle = {Proceedings of the 10th International Conference on Multimodal Interfaces},
pages = {237--240},
numpages = {4},
keywords = {multimodal fusion, stream reliability, audio-visual speech recognition, multi-stream hmm},
location = {Chania, Crete, Greece},
series = {ICMI '08}
}

@inproceedings{baltrusaitis2013dimensional,
  author={T. {Baltru\u{s}aitis} and N. {Banda} and P. {Robinson}},
  booktitle={2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)}, 
  title={Dimensional affect recognition using Continuous Conditional Random Fields}, 
  year={2013},
  volume={},
  number={},
  pages={1-8},}
  
@incollection{gao2015are,
title = {Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question},
author = {Gao, Haoyuan and Mao, Junhua and Zhou, Jie and Huang, Zhiheng and Wang, Lei and Xu, Wei},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2296--2304},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5641-are-you-talking-to-a-machine-dataset-and-methods-for-multilingual-image-question.pdf}
}

@inproceedings{malinowski2015ask,
  author={M. {Malinowski} and M. {Rohrbach} and M. {Fritz}},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Ask Your Neurons: A Neural-Based Approach to Answering Questions about Images}, 
  year={2015},
  volume={},
  number={},
  pages={1-9},}
 
@InProceedings{xu2016ask,
author={Xu, Huijuan
and Saenko, Kate},
editor={Leibe, Bastian
and Matas, Jiri
and Sebe, Nicu
and Welling, Max},
title={Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering},
booktitle={Computer Vision -- ECCV 2016},
year={2016},
publisher={Springer International Publishing},
address={Cham},
pages={451--466},
isbn={978-3-319-46478-7}
}

@article{neverova2016moddrop,
  author={N. {Neverova} and C. {Wolf} and G. {Taylor} and F. {Nebout}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={ModDrop: Adaptive Multi-Modal Gesture Recognition}, 
  year={2016},
  volume={38},
  number={8},
  pages={1692--1706},}

@article{kahou2015EmoNets,
  title={EmoNets: Multimodal deep learning approaches for emotion recognition in video},
  author={S. Kahou and Xavier Bouthillier and Pascal Lamblin and Çaglar G{\"u}lçehre and Vincent Michalski and Kishore Reddy Konda and S{\'e}bastien Jean and Pierre Froumenty and Yann Dauphin and Nicolas Boulanger-Lewandowski and Raul Chandias Ferrari and M. Mirza and David Warde-Farley and Aaron C. Courville and Pascal Vincent and R. Memisevic and C. Pal and Yoshua Bengio},
  journal={Journal on Multimodal User Interfaces},
  year={2015},
  volume={10},
  pages={99--111}
}

@inproceedings{nojavanasghari2016deep,
author = {Nojavanasghari, Behnaz and Gopinath, Deepak and Koushik, Jayanth and Baltru\v{s}aitis, Tadas and Morency, Louis-Philippe},
title = {Deep Multimodal Fusion for Persuasiveness Prediction},
year = {2016},
isbn = {9781450345569},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993148.2993176},
doi = {10.1145/2993148.2993176},
booktitle = {Proceedings of the 18th ACM International Conference on Multimodal Interaction},
pages = {284--288},
numpages = {5},
keywords = {Persuasiveness, deep neural networks, multimodal fusion},
location = {Tokyo, Japan},
series = {ICMI '16}
}

@inproceedings{jin2016video,
author = {Jin, Qin and Liang, Junwei},
title = {Video Description Generation Using Audio and Visual Cues},
year = {2016},
isbn = {9781450343596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2911996.2912043},
doi = {10.1145/2911996.2912043},
booktitle = {Proceedings of the 2016 ACM on International Conference on Multimedia Retrieval},
pages = {239--242},
numpages = {4},
keywords = {image caption, video description, audio analysis, deep neural networks},
location = {New York, New York, USA},
series = {ICMR '16}
}

@inproceedings{venugopalan2016improving,
    title = {Improving {LSTM}-based Video Description with Linguistic Knowledge Mined from Text},
    author = {Venugopalan, Subhashini  and
      Hendricks, Lisa Anne  and
      Mooney, Raymond  and
      Saenko, Kate},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    month = nov,
    year = {2016},
    address = {Austin, Texas},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D16-1204},
    doi = {10.18653/v1/D16-1204},
    pages = {1961--1966},
}
  
@InProceedings{andrew2013deep,
  title = 	 {Deep Canonical Correlation Analysis},
  author = 	 {Galen Andrew and Raman Arora and Jeff Bilmes and Karen Livescu},
  pages = 	 {1247--1255},
  year = 	 {2013},
  editor = 	 {Sanjoy Dasgupta and David McAllester},
  volume = 	 {28},
  booktitle = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/andrew13.pdf},
  url = 	 {http://proceedings.mlr.press/v28/andrew13.html},
}

@inproceedings{vendrov2016order,
	author = {Vendrov, Ivan and Kiros, Ryan and Fidler, Sanja and Urtasun, Raquel},
	year = {2016},
	pages = {},
	title = {Order-Embeddings of Images and Language},
	booktitle = {Proc. Int. Conf. Learn. Representations}
}

@inproceedings{bookscorpus,
author = {Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
title = {Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books},
year = {2015},
isbn = {9781467383912},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICCV.2015.11},
doi = {10.1109/ICCV.2015.11},
abstract = {Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in the current datasets. To align movies and books we propose a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.},
booktitle = {Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV)},
pages = {19–27},
numpages = {9},
series = {ICCV '15}
}
  
@article{malinowski2017ask,
author = {Malinowski, Mateusz
and Rohrbach, Marcus
and Fritz, Mario},
year = {2017},
title = {Ask Your Neurons: A Deep Learning Approach to Visual Question Answering},
journal = {International Journal of Computer Vision},
volume = {125},
url = {https://doi.org/10.1007/s11263-017-1038-2},
pages ={110--135}
}

@inproceedings{fukui2016multimodal,
    title = {Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding},
    author = {Fukui, Akira  and
      Park, Dong Huk  and
      Yang, Daylen  and
      Rohrbach, Anna  and
      Darrell, Trevor  and
      Rohrbach, Marcus},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    month = {nov},
    year = {2016},
    address = {Austin, Texas},
    publisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/D16-1044},
    doi = {10.18653/v1/D16-1044},
    pages = {457--468},
}

@incollection{ren2015exploring,
title = {Exploring Models and Data for Image Question Answering},
author = {Ren, Mengye and Kiros, Ryan and Zemel, Richard},
booktitle = {Advances in Neural Information Processing Systems 28},
editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
pages = {2953--2961},
year = {2015},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5640-exploring-models-and-data-for-image-question-answering.pdf}
}

@incollection{lu2016hierarchical,
	title = {Hierarchical Question-Image Co-Attention for Visual Question Answering},
	author = {Lu, Jiasen and Yang, Jianwei and Batra, Dhruv and Parikh, Devi},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {289--297},
	year = {2016},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6202-hierarchical-question-image-co-attention-for-visual-question-answering.pdf}
}

@InProceedings{gao2019dynamic,
	author = {Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven C. H. and Wang, Xiaogang and Li, Hongsheng},
	title = {Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	month = {June},
	year = {2019},
	pages={6639--6648},	
} 

@InProceedings{lin2014microsoft,
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Dollar, Piotr and Zitnick, Larry},
	title = {Microsoft COCO: Common Objects in Context},
	booktitle = {ECCV},
	year = {2014},
	month = {September},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	publisher = {European Conference on Computer Vision},
	url = {https://www.microsoft.com/en-us/research/publication/microsoft-coco-common-objects-in-context/},
	edition = {ECCV},
}

@InProceedings{kafle2017tdiuc,	
	author={K. {Kafle} and C. {Kanan}},
	booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
	title={An Analysis of Visual Question Answering Algorithms}, 
	year={2017},
	volume={},
	number={},
	pages={1983--1991},}
	
@article{Xie2016,
  title={Aggregated Residual Transformations for Deep Neural Networks},
  author={Saining Xie and Ross Girshick and Piotr Dollár and Zhuowen Tu and Kaiming He},
  journal={arXiv preprint arXiv:1611.05431},
  year={2016}
}

@misc{kingma2014adam,
    title={Adam: A Method for Stochastic Optimization},
    author={Diederik P. Kingma and Jimmy Ba},
    year={2014},
    eprint={1412.6980},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}